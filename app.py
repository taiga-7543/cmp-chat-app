from flask import Flask, request, jsonify, render_template, Response, redirect, url_for, session
from flask_httpauth import HTTPBasicAuth
from google import genai
from google.genai import types
import json
import os
import asyncio
import time
import re
from datetime import datetime
import tempfile
import hashlib
import base64
import gc
from dotenv import load_dotenv
import io
import mimetypes
from werkzeug.utils import secure_filename
from google.cloud import aiplatform
# RAGã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä¸€æ™‚çš„ã«ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
# from google.cloud.aiplatform_v1 import RagDataServiceClient
# from google.cloud.aiplatform_v1.types import ListRagFilesRequest, ImportRagFilesRequest, RagFile
from google.cloud import storage
import uuid
import docx
from pptx import Presentation  # PowerPointå‡¦ç†ç”¨
from urllib.parse import unquote

# Google DriveåŒæœŸæ©Ÿèƒ½ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from drive_sync_integration import init_drive_sync, get_drive_sync

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

app = Flask(__name__)
app.secret_key = os.environ.get('SECRET_KEY', 'your-secret-key-change-this')
auth = HTTPBasicAuth()

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', 'dotd-development-division')
RAG_CORPUS = os.environ.get('RAG_CORPUS', f'projects/{PROJECT_ID}/locations/us-central1/ragCorpora/3458764513820540928')
GEMINI_MODEL = os.environ.get('GEMINI_MODEL', 'gemini-2.5-flash')

# èªè¨¼è¨­å®š
AUTH_USERNAME = os.environ.get('AUTH_USERNAME', 'u7F3kL9pQ2zX')
AUTH_PASSWORD = os.environ.get('AUTH_PASSWORD', 's8Vn2BqT5wXc')

@auth.verify_password
def verify_password(username, password):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åã¨ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œè¨¼"""
    if username == AUTH_USERNAME and password == AUTH_PASSWORD:
        return username
    return None

@auth.error_handler
def auth_error(status):
    """èªè¨¼ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    return jsonify({'error': 'ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸã€‚æ­£ã—ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼åã¨ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚'}), status

def fix_base64_padding(data):
    """Base64ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä¿®æ­£"""
    missing_padding = len(data) % 4
    if missing_padding:
        data += '=' * (4 - missing_padding)
    return data

def validate_and_fix_private_key(private_key):
    """ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚­ãƒ¼ã®å½¢å¼ã‚’æ¤œè¨¼ãƒ»ä¿®æ­£"""
    if not private_key:
        return private_key
    
    # PEMå½¢å¼ã®ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ»ãƒ•ãƒƒã‚¿ãƒ¼ã‚’ç¢ºèª
    if not private_key.startswith('-----BEGIN PRIVATE KEY-----'):
        return private_key
    
    # PEMå½¢å¼ã®ã‚­ãƒ¼ã‚’è¡Œã«åˆ†å‰²
    lines = private_key.split('\n')
    if len(lines) < 3:
        return private_key
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ãƒ•ãƒƒã‚¿ãƒ¼ã‚’é™¤ã„ãŸBase64éƒ¨åˆ†ã‚’å–å¾—
    base64_lines = []
    for line in lines[1:-1]:  # ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ãƒ•ãƒƒã‚¿ãƒ¼ã‚’é™¤ã
        line = line.strip()
        if line and not line.startswith('-----'):
            base64_lines.append(line)
    
    if not base64_lines:
        return private_key
    
    # Base64æ–‡å­—åˆ—ã‚’çµåˆ
    base64_data = ''.join(base64_lines)
    
    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä¿®æ­£
    try:
        fixed_base64 = fix_base64_padding(base64_data)
        # Base64ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’ãƒ†ã‚¹ãƒˆ
        base64.b64decode(fixed_base64)
        
        # ä¿®æ­£ã•ã‚ŒãŸã‚­ãƒ¼ã‚’å†æ§‹ç¯‰
        fixed_private_key = '-----BEGIN PRIVATE KEY-----\n'
        # 64æ–‡å­—ãšã¤æ”¹è¡Œ
        for i in range(0, len(fixed_base64), 64):
            fixed_private_key += fixed_base64[i:i+64] + '\n'
        fixed_private_key += '-----END PRIVATE KEY-----'
        
        return fixed_private_key
        
    except Exception as e:
        print(f"Warning: Could not fix private key Base64 padding: {e}")
        return private_key

def setup_google_auth():
    """Google Cloudèªè¨¼ã‚’è¨­å®š"""
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã®JSONã‚’èª­ã¿è¾¼ã‚€
    credentials_json = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS_JSON')
    if credentials_json:
        try:
            # JSONã®å¦¥å½“æ€§ã‚’ç¢ºèª
            parsed_json = json.loads(credentials_json)
            
            # å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            required_fields = ['type', 'project_id', 'private_key_id', 'private_key', 'client_email']
            missing_fields = [field for field in required_fields if field not in parsed_json]
            
            if missing_fields:
                print(f"Warning: Missing required fields in Google Cloud credentials: {missing_fields}")
                return
            
            # ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚­ãƒ¼ã®Base64ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä¿®æ­£
            if 'private_key' in parsed_json:
                original_key = parsed_json['private_key']
                fixed_key = validate_and_fix_private_key(original_key)
                if fixed_key != original_key:
                    print("INFO: Fixed private key Base64 padding")
                    parsed_json['private_key'] = fixed_key
            
            # JSONã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚“ã§èªè¨¼è¨­å®š
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                json.dump(parsed_json, f, indent=2)
                os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = f.name
                print(f"Google Cloud credentials file created: {f.name}")
                
        except json.JSONDecodeError as e:
            print(f"Error: Invalid JSON in GOOGLE_APPLICATION_CREDENTIALS_JSON: {e}")
            print("Please check the format of your Google Cloud credentials JSON.")
            return
        except Exception as e:
            print(f"Error setting up Google Cloud authentication: {e}")
            return
    
    # æ—¢å­˜ã®GOOGLE_APPLICATION_CREDENTIALSãŒã‚ã‚‹å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
    if not os.environ.get('GOOGLE_APPLICATION_CREDENTIALS') and not credentials_json:
        print("Warning: Google Cloudèªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã®ã„ãšã‚Œã‹ã‚’è¨­å®šã—ã¦ãã ã•ã„:")
        print("- GOOGLE_APPLICATION_CREDENTIALS: ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
        print("- GOOGLE_APPLICATION_CREDENTIALS_JSON: ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã®JSONæ–‡å­—åˆ—")
    elif os.environ.get('GOOGLE_APPLICATION_CREDENTIALS'):
        print(f"Using existing Google Cloud credentials file: {os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')}")

# èªè¨¼è¨­å®šã‚’åˆæœŸåŒ–
setup_google_auth()

# Google DriveåŒæœŸã‚’åˆæœŸåŒ–
CLIENT_SECRETS_FILE = os.environ.get('GOOGLE_OAUTH_CLIENT_SECRETS_FILE', 'config/client_secrets.json')
CLOUD_STORAGE_BUCKET = os.environ.get('CLOUD_STORAGE_BUCKET', 'dotd-cmp-wg-search')
DRIVE_FOLDER_NAME = os.environ.get('DRIVE_SYNC_FOLDER_NAME', 'RAG Documents')

if os.path.exists(CLIENT_SECRETS_FILE):
    init_drive_sync(PROJECT_ID, CLIENT_SECRETS_FILE, CLOUD_STORAGE_BUCKET)
    print(f"Google DriveåŒæœŸæ©Ÿèƒ½ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
else:
    print(f"è­¦å‘Š: {CLIENT_SECRETS_FILE} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Google DriveåŒæœŸæ©Ÿèƒ½ã¯ç„¡åŠ¹ã§ã™ã€‚")

# ãƒ¡ãƒ¢ãƒªç®¡ç†ã®è¨­å®š
gc.set_threshold(700, 10, 10)  # ã‚ˆã‚Šç©æ¥µçš„ãªã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³

def create_rag_client():
    """RAGã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ"""
    client = genai.Client(
        vertexai=True,
        project=PROJECT_ID,
        location="global",
    )
    return client

def extract_date_from_filename(filename):
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡ºã™ã‚‹ï¼ˆ_yyyymmddå½¢å¼ã¾ãŸã¯yyyymmddå½¢å¼ï¼‰"""
    if not filename:
        return None
    
    # yyyymmddå½¢å¼ã®æ—¥ä»˜ã‚’æ¤œç´¢ï¼ˆã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚ã‚Šã¾ãŸã¯ãªã—ï¼‰
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: _yyyymmddå½¢å¼ï¼ˆã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚ã‚Šï¼‰
    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: yyyymmddå½¢å¼ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã®å…ˆé ­ã¾ãŸã¯åŒºåˆ‡ã‚Šæ–‡å­—ã®å¾Œï¼‰
    date_patterns = [
        r'_(\d{8})(?:\.|_|$)',  # _yyyymmddå½¢å¼
        r'(?:^|[^\d])(\d{8})(?:[^\d]|$)',  # yyyymmddå½¢å¼ï¼ˆå‰å¾Œã«æ•°å­—ä»¥å¤–ã®æ–‡å­—ï¼‰
    ]
    
    for pattern in date_patterns:
        match = re.search(pattern, filename)
        if match:
            date_str = match.group(1)
            try:
                # æ—¥ä»˜ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                date_obj = datetime.strptime(date_str, '%Y%m%d')
                
                # å¦¥å½“ãªæ—¥ä»˜ç¯„å›²ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ1900å¹´ã€œ2100å¹´ï¼‰
                if 1900 <= date_obj.year <= 2100:
                    return date_obj
            except ValueError:
                # ç„¡åŠ¹ãªæ—¥ä»˜ã®å ´åˆã¯æ¬¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
                continue
    
    return None

def sort_sources_by_date(sources):
    """å‡ºå…¸æƒ…å ±ã‚’æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„æ—¥ä»˜ã‚’å„ªå…ˆï¼‰"""
    def get_sort_key(source):
        title = source.get('title', '')
        uri = source.get('uri', '')
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã¨URIã®ä¸¡æ–¹ã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡ºã‚’è©¦ã¿ã‚‹
        date_from_title = extract_date_from_filename(title)
        date_from_uri = extract_date_from_filename(uri)
        
        # ã‚ˆã‚Šæ–°ã—ã„æ—¥ä»˜ã‚’ä½¿ç”¨
        extracted_date = None
        if date_from_title and date_from_uri:
            extracted_date = max(date_from_title, date_from_uri)
        elif date_from_title:
            extracted_date = date_from_title
        elif date_from_uri:
            extracted_date = date_from_uri
        
        # æ—¥ä»˜ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€ã‚‚å¤ã„æ—¥ä»˜ã¨ã—ã¦æ‰±ã†
        if extracted_date is None:
            extracted_date = datetime(1900, 1, 1)
        
        # æ–°ã—ã„æ—¥ä»˜ã‚’å„ªå…ˆã™ã‚‹ãŸã‚ã€æ—¥ä»˜ã‚’é€†é †ã§ã‚½ãƒ¼ãƒˆ
        return (-extracted_date.timestamp(), title.lower())
    
    return sorted(sources, key=get_sort_key)

def convert_grounding_metadata_to_dict(grounding_metadata):
    """GroundingMetadataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¾æ›¸ã«å¤‰æ›ã—ã€æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆ"""
    if not grounding_metadata:
        print("DEBUG: grounding_metadata is None")
        return None
    
    print(f"DEBUG: grounding_metadata type: {type(grounding_metadata)}")
    print(f"DEBUG: grounding_metadata attributes: {dir(grounding_metadata)}")
    
    result = {}
    
    # grounding_chunksã‚’å‡¦ç†
    if hasattr(grounding_metadata, 'grounding_chunks') and grounding_metadata.grounding_chunks:
        print(f"DEBUG: Found {len(grounding_metadata.grounding_chunks)} grounding chunks")
        unsorted_chunks = []
        
        for i, chunk in enumerate(grounding_metadata.grounding_chunks):
            print(f"DEBUG: Processing chunk {i}: {type(chunk)}")
            print(f"DEBUG: Chunk attributes: {dir(chunk)}")
            
            chunk_dict = {}
            
            # retrieved_contextã‹ã‚‰titleã¨uriã‚’å–å¾—
            if hasattr(chunk, 'retrieved_context') and chunk.retrieved_context:
                print(f"DEBUG: Found retrieved_context: {type(chunk.retrieved_context)}")
                retrieved_context = chunk.retrieved_context
                
                # titleã‚’å–å¾—
                if hasattr(retrieved_context, 'title') and retrieved_context.title:
                    chunk_dict['title'] = retrieved_context.title
                    print(f"DEBUG: Found title in retrieved_context: {retrieved_context.title}")
                
                # uriã‚’å–å¾—
                if hasattr(retrieved_context, 'uri') and retrieved_context.uri:
                    chunk_dict['uri'] = retrieved_context.uri
                    print(f"DEBUG: Found uri in retrieved_context: {retrieved_context.uri}")
            
            # webãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚‚ç¢ºèªï¼ˆå¿µã®ãŸã‚ï¼‰
            if hasattr(chunk, 'web') and chunk.web:
                print(f"DEBUG: Found web property: {chunk.web}")
                if not chunk_dict.get('title') and hasattr(chunk.web, 'title'):
                    chunk_dict['title'] = chunk.web.title
                if not chunk_dict.get('uri') and hasattr(chunk.web, 'uri'):
                    chunk_dict['uri'] = chunk.web.uri
            
            # ç›´æ¥çš„ãªtitle/uriãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚‚ç¢ºèª
            if not chunk_dict.get('title') and hasattr(chunk, 'title'):
                chunk_dict['title'] = chunk.title
                print(f"DEBUG: Found direct title: {chunk.title}")
            if not chunk_dict.get('uri') and hasattr(chunk, 'uri'):
                chunk_dict['uri'] = chunk.uri
                print(f"DEBUG: Found direct uri: {chunk.uri}")
            
            # æ—¥ä»˜æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
            if chunk_dict.get('title'):
                extracted_date = extract_date_from_filename(chunk_dict['title'])
                if extracted_date:
                    print(f"DEBUG: Extracted date from title '{chunk_dict['title']}': {extracted_date.strftime('%Y-%m-%d')}")
                else:
                    print(f"DEBUG: No date found in title '{chunk_dict['title']}'")
            
            unsorted_chunks.append(chunk_dict)
        
        # æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„æ—¥ä»˜ã‚’å„ªå…ˆï¼‰
        sorted_chunks = sort_sources_by_date(unsorted_chunks)
        result['grounding_chunks'] = sorted_chunks
        
        print(f"DEBUG: Sorted chunks by date:")
        for i, chunk in enumerate(sorted_chunks):
            title = chunk.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—')
            date = extract_date_from_filename(title)
            date_str = date.strftime('%Y-%m-%d') if date else 'æ—¥ä»˜ãªã—'
            print(f"  {i+1}. {title} ({date_str})")
        
    else:
        print("DEBUG: No grounding_chunks found")
    
    print(f"DEBUG: Final result: {result}")
    return result

def generate_plan_and_questions(user_message):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‹ã‚‰è¨ˆç”»ã¨é–¢é€£è³ªå•ã‚’ç”Ÿæˆ"""
    try:
        client = create_rag_client()
        
        planning_prompt = f"""
ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€åŒ…æ‹¬çš„ã§è©³ç´°ãªå›ç­”ã‚’æä¾›ã™ã‚‹ãŸã‚ã®è¨ˆç”»ã‚’ç«‹ã¦ã¦ãã ã•ã„ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {user_message}

ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

## èª¿æŸ»è¨ˆç”»
[ã“ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã®èª¿æŸ»è¨ˆç”»ã‚’ç°¡æ½”ã«èª¬æ˜]

## é–¢é€£è³ªå•ãƒªã‚¹ãƒˆ
1. [é–¢é€£è³ªå•1]
2. [é–¢é€£è³ªå•2]
3. [é–¢é€£è³ªå•3]
4. [é–¢é€£è³ªå•4]
5. [é–¢é€£è³ªå•5]

é–¢é€£è³ªå•ã¯ä»¥ä¸‹ã®è¦³ç‚¹ã‹ã‚‰ä½œæˆã—ã¦ãã ã•ã„ï¼š
- åŸºæœ¬çš„ãªå®šç¾©ã‚„æ¦‚å¿µ
- å…·ä½“çš„ãªäº‹ä¾‹ã‚„å¿œç”¨
- ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ
- æœ€æ–°ã®å‹•å‘ã‚„èª²é¡Œ
- é–¢é€£ã™ã‚‹æŠ€è¡“ã‚„æ‰‹æ³•

å„è³ªå•ã¯ç‹¬ç«‹ã—ã¦å›ç­”å¯èƒ½ã§ã€å…ƒã®è³ªå•ã®ç†è§£ã‚’æ·±ã‚ã‚‹ã‚‚ã®ã«ã—ã¦ãã ã•ã„ã€‚
"""
        
        contents = [
            types.Content(
                role="user",
                parts=[types.Part(text=planning_prompt)]
            )
        ]
        
        config = types.GenerateContentConfig(
            temperature=0.7,
            top_p=0.9,
            max_output_tokens=1000,
        )
        
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=contents,
            config=config,
        )
        
        if response and response.text:
            return response.text
        else:
            print("WARNING: Empty response from planning model")
            return f"""
## èª¿æŸ»è¨ˆç”»
{user_message}ã«ã¤ã„ã¦è©³ç´°ã«èª¿æŸ»ã—ã¾ã™ã€‚

## é–¢é€£è³ªå•ãƒªã‚¹ãƒˆ
1. {user_message}ã®åŸºæœ¬çš„ãªå®šç¾©ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ
2. {user_message}ã®å…·ä½“çš„ãªäº‹ä¾‹ã‚’æ•™ãˆã¦ãã ã•ã„
3. {user_message}ã®ãƒ¡ãƒªãƒƒãƒˆã¨ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ
4. {user_message}ã®æœ€æ–°ã®å‹•å‘ã¯ã©ã†ã§ã™ã‹ï¼Ÿ
5. {user_message}ã«é–¢é€£ã™ã‚‹æŠ€è¡“ã‚„æ‰‹æ³•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
"""
    except Exception as e:
        print(f"Error in generate_plan_and_questions: {e}")
        return f"""
## èª¿æŸ»è¨ˆç”»
{user_message}ã«ã¤ã„ã¦è©³ç´°ã«èª¿æŸ»ã—ã¾ã™ã€‚

## é–¢é€£è³ªå•ãƒªã‚¹ãƒˆ
1. {user_message}ã®åŸºæœ¬çš„ãªå®šç¾©ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ
2. {user_message}ã®å…·ä½“çš„ãªäº‹ä¾‹ã‚’æ•™ãˆã¦ãã ã•ã„
3. {user_message}ã®ãƒ¡ãƒªãƒƒãƒˆã¨ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ
4. {user_message}ã®æœ€æ–°ã®å‹•å‘ã¯ã©ã†ã§ã™ã‹ï¼Ÿ
5. {user_message}ã«é–¢é€£ã™ã‚‹æŠ€è¡“ã‚„æ‰‹æ³•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
"""

def execute_single_rag_query(question):
    """å˜ä¸€ã®RAGã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ"""
    try:
        client = create_rag_client()
        
        contents = [
            types.Content(
                role="user",
                parts=[types.Part(text=question)]
            )
        ]
        
        tools = [
            types.Tool(
                retrieval=types.Retrieval(
                    vertex_rag_store=types.VertexRagStore(
                        rag_resources=[
                            types.VertexRagStoreRagResource(
                                rag_corpus=RAG_CORPUS
                            )
                        ],
                    )
                )
            )
        ]

        config = types.GenerateContentConfig(
            temperature=0.8,
            top_p=0.9,
            max_output_tokens=2000,
            tools=tools,
        )
        
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=contents,
            config=config,
        )
        
        # ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        grounding_metadata = None
        if hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]
            if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                grounding_metadata = candidate.grounding_metadata
        
        answer_text = response.text if response and response.text else "å›ç­”ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
        return answer_text, grounding_metadata
        
    except Exception as e:
        print(f"Error in execute_single_rag_query: {e}")
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", None

def synthesize_comprehensive_answer(user_message, plan_text, qa_results):
    """è¨ˆç”»ã¨å„è³ªå•ã®å›ç­”ã‚’çµ±åˆã—ã¦åŒ…æ‹¬çš„ãªå›ç­”ã‚’ç”Ÿæˆ"""
    client = create_rag_client()
    
    qa_text = "\n\n".join([f"**Q: {q}**\nA: {a}" for q, a in qa_results])
    
    synthesis_prompt = f"""
ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã™ã‚‹åŒ…æ‹¬çš„ã§è©³ç´°ãªå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

å…ƒã®è³ªå•: {user_message}

èª¿æŸ»è¨ˆç”»:
{plan_text}

é–¢é€£è³ªå•ã¨å›ç­”:
{qa_text}

ä»¥ä¸‹ã®è¦ä»¶ã«å¾“ã£ã¦å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
1. å…ƒã®è³ªå•ã«ç›´æ¥ç­”ãˆã‚‹
2. é–¢é€£è³ªå•ã®å›ç­”ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸæƒ…å ±ã‚’çµ±åˆã™ã‚‹
3. è«–ç†çš„ã§èª­ã¿ã‚„ã™ã„æ§‹æˆã«ã™ã‚‹
4. é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’å¼·èª¿ã™ã‚‹
5. å…·ä½“ä¾‹ãŒã‚ã‚Œã°å«ã‚ã‚‹
6. Markdownå½¢å¼ã§æ•´ç†ã™ã‚‹

å›ç­”ã¯ä»¥ä¸‹ã®æ§‹æˆã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ï¼š
- æ¦‚è¦ãƒ»å®šç¾©
- è©³ç´°èª¬æ˜
- å…·ä½“ä¾‹ãƒ»äº‹ä¾‹
- ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ
- æœ€æ–°å‹•å‘ãƒ»èª²é¡Œ
- ã¾ã¨ã‚
"""
    
    contents = [
        types.Content(
            role="user",
            parts=[types.Part(text=synthesis_prompt)]
        )
    ]
    
    config = types.GenerateContentConfig(
        temperature=0.7,
        top_p=0.9,
        max_output_tokens=4000,
    )
    
    response = client.models.generate_content(
        model=GEMINI_MODEL,
        contents=contents,
        config=config,
    )
    
    return response.text

def generate_deep_response(user_message):
    """æ·±æ˜ã‚Šæ©Ÿèƒ½ä»˜ãã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ"""
    try:
        # ã‚¹ãƒ†ãƒƒãƒ—1: è¨ˆç”»ç«‹ã¦ã¨é–¢é€£è³ªå•ç”Ÿæˆ
        yield {
            'chunk': '\n## ğŸ“‹ èª¿æŸ»è¨ˆç”»ã‚’ç«‹æ¡ˆä¸­...\n',
            'done': False,
            'grounding_metadata': None,
            'step': 'planning'
        }
        
        plan_text = generate_plan_and_questions(user_message)
        
        if not plan_text:
            plan_text = f"""
## èª¿æŸ»è¨ˆç”»
{user_message}ã«ã¤ã„ã¦è©³ç´°ã«èª¿æŸ»ã—ã¾ã™ã€‚

## é–¢é€£è³ªå•ãƒªã‚¹ãƒˆ
1. {user_message}ã®åŸºæœ¬çš„ãªå®šç¾©ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ
2. {user_message}ã®å…·ä½“çš„ãªäº‹ä¾‹ã‚’æ•™ãˆã¦ãã ã•ã„
3. {user_message}ã®ãƒ¡ãƒªãƒƒãƒˆã¨ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ
4. {user_message}ã®æœ€æ–°ã®å‹•å‘ã¯ã©ã†ã§ã™ã‹ï¼Ÿ
5. {user_message}ã«é–¢é€£ã™ã‚‹æŠ€è¡“ã‚„æ‰‹æ³•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
"""
        
        yield {
            'chunk': f'\n{plan_text}\n\n## ğŸ” è©³ç´°èª¿æŸ»ã‚’é–‹å§‹...\n',
            'done': False,
            'grounding_metadata': None,
            'step': 'plan_complete'
        }
        
        # é–¢é€£è³ªå•ã‚’æŠ½å‡º
        questions = []
        try:
            lines = plan_text.split('\n')
            in_questions_section = False
            
            for line in lines:
                if 'é–¢é€£è³ªå•' in line:
                    in_questions_section = True
                    continue
                if in_questions_section and line.strip():
                    if line.strip().startswith(('1.', '2.', '3.', '4.', '5.')):
                        question = line.strip()[2:].strip()
                        if question and question not in questions:
                            questions.append(question)
        except Exception as e:
            print(f"Error extracting questions: {e}")
            questions = [
                f"{user_message}ã®åŸºæœ¬çš„ãªå®šç¾©ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
                f"{user_message}ã®å…·ä½“çš„ãªäº‹ä¾‹ã‚’æ•™ãˆã¦ãã ã•ã„",
                f"{user_message}ã®ãƒ¡ãƒªãƒƒãƒˆã¨ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ",
                f"{user_message}ã®æœ€æ–°ã®å‹•å‘ã¯ã©ã†ã§ã™ã‹ï¼Ÿ",
                f"{user_message}ã«é–¢é€£ã™ã‚‹æŠ€è¡“ã‚„æ‰‹æ³•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
            ]
        
        # è³ªå•ãŒå°‘ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if len(questions) < 3:
            questions = [
                f"{user_message}ã®åŸºæœ¬çš„ãªå®šç¾©ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
                f"{user_message}ã®å…·ä½“çš„ãªäº‹ä¾‹ã‚’æ•™ãˆã¦ãã ã•ã„",
                f"{user_message}ã®ãƒ¡ãƒªãƒƒãƒˆã¨ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ",
                f"{user_message}ã®æœ€æ–°ã®å‹•å‘ã¯ã©ã†ã§ã™ã‹ï¼Ÿ",
                f"{user_message}ã«é–¢é€£ã™ã‚‹æŠ€è¡“ã‚„æ‰‹æ³•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
            ]
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: å„é–¢é€£è³ªå•ã‚’é †æ¬¡å®Ÿè¡Œ
        qa_results = []
        all_grounding_metadata = []
        all_unique_sources = {}  # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚è¾æ›¸ã§ç®¡ç†
        
        for i, question in enumerate(questions[:5], 1):
            yield {
                'chunk': f'\n### ğŸ” è³ªå• {i}: {question}\nèª¿æŸ»ä¸­...\n',
                'done': False,
                'grounding_metadata': None,
                'step': f'query_{i}'
            }
            
            answer, grounding_metadata = execute_single_rag_query(question)
            qa_results.append((question, answer))
            
            # å‡ºå…¸æƒ…å ±ã‚’å‡¦ç†
            converted_metadata = None
            if grounding_metadata:
                all_grounding_metadata.append(grounding_metadata)
                converted_metadata = convert_grounding_metadata_to_dict(grounding_metadata)
                
                # å‡ºå…¸æƒ…å ±ã‚’çµ±åˆï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ï¼‰
                if converted_metadata and 'grounding_chunks' in converted_metadata:
                    for chunk in converted_metadata['grounding_chunks']:
                        if chunk.get('uri'):
                            all_unique_sources[chunk['uri']] = {
                                'title': chunk.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—'),
                                'uri': chunk['uri']
                            }
            
            # å›ç­”ã‚’é€ä¿¡
            yield {
                'chunk': f'\n**ğŸ’¡ å›ç­” {i}:** {answer}\n',
                'done': False,
                'grounding_metadata': converted_metadata,
                'step': f'answer_{i}'
            }
            
            # å„è³ªå•ã®å‡ºå…¸æƒ…å ±ã‚’å€‹åˆ¥ã«è¡¨ç¤º
            if converted_metadata and 'grounding_chunks' in converted_metadata:
                # å‡ºå…¸æƒ…å ±ã‚’æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆ
                sorted_chunks = sort_sources_by_date(converted_metadata['grounding_chunks'])
                
                sources_text = '\n**ğŸ“š ã“ã®å›ç­”ã®å‡ºå…¸:**\n'
                for j, chunk in enumerate(sorted_chunks, 1):
                    title = chunk.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—')
                    uri = chunk.get('uri', '')
                    
                    # æ—¥ä»˜æƒ…å ±ã‚’è¡¨ç¤ºã«å«ã‚ã‚‹
                    extracted_date = extract_date_from_filename(title)
                    date_info = f" ({extracted_date.strftime('%Y-%m-%d')})" if extracted_date else ""
                    
                    sources_text += f'   {j}. {title}{date_info}\n'
                    if uri:
                        sources_text += f'      ğŸ“ {uri}\n'
                sources_text += '\n'
                
                yield {
                    'chunk': sources_text,
                    'done': False,
                    'grounding_metadata': None,
                    'step': f'sources_{i}'
                }
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: åŒ…æ‹¬çš„ãªå›ç­”ã®çµ±åˆ
        yield {
            'chunk': '\n## ğŸ“ åŒ…æ‹¬çš„ãªå›ç­”ã‚’ä½œæˆä¸­...\n',
            'done': False,
            'grounding_metadata': None,
            'step': 'synthesizing'
        }
        
        comprehensive_answer = synthesize_comprehensive_answer(user_message, plan_text, qa_results)
        
        yield {
            'chunk': f'\n## ğŸ¯ åŒ…æ‹¬çš„ãªå›ç­”\n\n{comprehensive_answer}\n',
            'done': False,
            'grounding_metadata': None,
            'step': 'synthesis_complete'
        }
        
        # å…¨ã¦ã®å‡ºå…¸æƒ…å ±ã‚’çµ±åˆã—ã¦è¡¨ç¤º
        if all_unique_sources:
            # æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ã‹ã‚‰è¡¨ç¤º
            sorted_unique_sources = sort_sources_by_date(list(all_unique_sources.values()))
            
            yield {
                'chunk': '\n## ğŸ“š å…¨ä½“ã®å‡ºå…¸æƒ…å ±\n\n',
                'done': False,
                'grounding_metadata': None,
                'step': 'all_sources_header'
            }
            
            sources_summary = ''
            for i, source_info in enumerate(sorted_unique_sources, 1):
                title = source_info.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—')
                uri = source_info.get('uri', '')
                
                # æ—¥ä»˜æƒ…å ±ã‚’è¡¨ç¤ºã«å«ã‚ã‚‹
                extracted_date = extract_date_from_filename(title)
                date_info = f" ({extracted_date.strftime('%Y-%m-%d')})" if extracted_date else ""
                
                sources_summary += f'**{i}. {title}{date_info}**\n'
                sources_summary += f'   ğŸ“ {uri}\n\n'
            
            yield {
                'chunk': sources_summary,
                'done': False,
                'grounding_metadata': None,
                'step': 'all_sources_list'
            }
        
        # æœ€çµ‚çš„ãªå‡ºå…¸æƒ…å ±ã‚’çµ±åˆï¼ˆJSONã¨ã—ã¦é€ä¿¡ï¼‰
        final_grounding_metadata = None
        if all_grounding_metadata:
            # å…¨ã¦ã®å‡ºå…¸æƒ…å ±ã‚’çµ±åˆã—ã€æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆ
            sorted_unique_sources = sort_sources_by_date(list(all_unique_sources.values()))
            final_grounding_metadata = {
                'grounding_chunks': sorted_unique_sources
            }
        
        yield {
            'chunk': '',
            'done': True,
            'grounding_metadata': final_grounding_metadata,
            'step': 'complete'
        }
        
    except Exception as e:
        print(f"Deep response generation error: {e}")
        yield {
            'chunk': f'\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\né€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å›ç­”ã‚’è©¦ã¿ã¾ã™...\n',
            'done': False,
            'grounding_metadata': None,
            'step': 'error'
        }
        
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        try:
            for chunk_data in generate_response(user_message):
                yield chunk_data
        except Exception as fallback_error:
            print(f"Fallback error: {fallback_error}")
            yield {
                'chunk': f'\nâŒ å›ç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(fallback_error)}\n',
                'done': True,
                'grounding_metadata': None,
                'step': 'fallback_error'
            }

def generate_response(user_message):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¯¾ã—ã¦RAGã‚’ä½¿ç”¨ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ"""
    client = create_rag_client()
    
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part(text=user_message)
            ]
        )
    ]
    tools = [
        types.Tool(
            retrieval=types.Retrieval(
                vertex_rag_store=types.VertexRagStore(
                    rag_resources=[
                        types.VertexRagStoreRagResource(
                            rag_corpus=RAG_CORPUS
                        )
                    ],
                )
            )
        )
    ]

    # GenerateContentConfigã‚’ä½œæˆ
    config_params = {
        'temperature': 1,
        'top_p': 1,
        'seed': 0,
        'max_output_tokens': 65535,
        'safety_settings': [
            types.SafetySetting(
                category="HARM_CATEGORY_HATE_SPEECH",
                threshold="OFF"
            ),
            types.SafetySetting(
                category="HARM_CATEGORY_DANGEROUS_CONTENT",
                threshold="OFF"
            ),
            types.SafetySetting(
                category="HARM_CATEGORY_SEXUALLY_EXPLICIT",
                threshold="OFF"
            ),
            types.SafetySetting(
                category="HARM_CATEGORY_HARASSMENT",
                threshold="OFF"
            )
        ],
        'tools': tools,
    }
    
    # ThinkingConfigãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿è¿½åŠ 
    try:
        # ã‚ˆã‚Šè©³ç´°ãªãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè£…
        thinking_config_available = False
        try:
            # ThinkingConfigã‚¯ãƒ©ã‚¹ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if hasattr(types, 'ThinkingConfig'):
                # å®Ÿéš›ã«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã§ãã‚‹ã‹ãƒ†ã‚¹ãƒˆ
                test_config = types.ThinkingConfig(thinking_budget=-1)
                thinking_config_available = True
                print("DEBUG: ThinkingConfig is available")
            else:
                # ä»£æ›¿çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦è¡Œ
                try:
                    from google.genai.types import ThinkingConfig
                    test_config = ThinkingConfig(thinking_budget=-1)
                    thinking_config_available = True
                    print("DEBUG: ThinkingConfig imported directly")
                except ImportError:
                    print("DEBUG: ThinkingConfig not available via direct import")
        except (AttributeError, TypeError, ValueError) as e:
            print(f"DEBUG: ThinkingConfig not available: {e}")
            thinking_config_available = False
        
        if thinking_config_available:
            config_params['thinking_config'] = types.ThinkingConfig(
                thinking_budget=-1,
            )
            print("DEBUG: Added ThinkingConfig to config")
        else:
            print("DEBUG: Skipping ThinkingConfig - not available")
    except Exception as e:
        print(f"DEBUG: Error checking ThinkingConfig: {e}")
        pass  # ThinkingConfigãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ç„¡è¦–
    
    # å®‰å…¨ã«GenerateContentConfigã‚’ä½œæˆ
    try:
        generate_content_config = types.GenerateContentConfig(**config_params)
        print("DEBUG: GenerateContentConfig created successfully")
    except Exception as e:
        print(f"DEBUG: Error creating GenerateContentConfig: {e}")
        # ThinkingConfigã‚’é™¤å¤–ã—ã¦å†è©¦è¡Œ
        if 'thinking_config' in config_params:
            del config_params['thinking_config']
            print("DEBUG: Retrying without ThinkingConfig")
            generate_content_config = types.GenerateContentConfig(**config_params)

    full_response = ""
    grounding_metadata = None
    
    print(f"DEBUG: Starting generation for message: {user_message}")
    
    for chunk in client.models.generate_content_stream(
        model=GEMINI_MODEL,
        contents=contents,
        config=generate_content_config,
    ):
        print(f"DEBUG: Raw chunk: {chunk}")
        
        if not chunk.candidates or not chunk.candidates[0].content or not chunk.candidates[0].content.parts:
            print("DEBUG: Skipping chunk - no content")
            continue
        
        print(f"DEBUG: Chunk type: {type(chunk)}")
        print(f"DEBUG: Chunk attributes: {[attr for attr in dir(chunk) if not attr.startswith('_')]}")
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’è“„ç©
        full_response += chunk.text
        print(f"DEBUG: Added text: {chunk.text[:100]}...")
        
        # å…¨ã¦ã®å¯èƒ½ãªå ´æ‰€ã§ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã™
        candidate = chunk.candidates[0]
        print(f"DEBUG: Candidate attributes: {[attr for attr in dir(candidate) if not attr.startswith('_')]}")
        
        # å€™è£œ1: candidate.grounding_metadata
        if hasattr(candidate, 'grounding_metadata'):
            print(f"DEBUG: candidate.grounding_metadata exists: {candidate.grounding_metadata}")
            if candidate.grounding_metadata:
                print("DEBUG: Found grounding_metadata in candidate")
                grounding_metadata = candidate.grounding_metadata
        
        # å€™è£œ2: chunk.grounding_metadata
        if hasattr(chunk, 'grounding_metadata'):
            print(f"DEBUG: chunk.grounding_metadata exists: {chunk.grounding_metadata}")
            if chunk.grounding_metadata:
                print("DEBUG: Found grounding_metadata in chunk")
                grounding_metadata = chunk.grounding_metadata
            
        # å€™è£œ3: candidate.content.grounding_metadata
        if hasattr(candidate.content, 'grounding_metadata'):
            print(f"DEBUG: candidate.content.grounding_metadata exists: {candidate.content.grounding_metadata}")
            if candidate.content.grounding_metadata:
                print("DEBUG: Found grounding_metadata in candidate.content")
                grounding_metadata = candidate.content.grounding_metadata
        
        # å€™è£œ4: å…¨ä½“ã®æ§‹é€ ã‚’ç¢ºèª
        print(f"DEBUG: Full candidate structure:")
        for attr in dir(candidate):
            if not attr.startswith('_'):
                try:
                    value = getattr(candidate, attr)
                    print(f"  {attr}: {type(value)} - {value}")
                except:
                    print(f"  {attr}: <error accessing>")
        
        yield {
            'chunk': chunk.text,
            'done': False,
            'grounding_metadata': None
        }
    
    print(f"DEBUG: Final grounding_metadata: {grounding_metadata}")
    
    # æœ€å¾Œã«å‡ºå…¸æƒ…å ±ã‚’é€ä¿¡ï¼ˆè¾æ›¸å½¢å¼ã«å¤‰æ›ï¼‰
    converted_metadata = convert_grounding_metadata_to_dict(grounding_metadata)
    print(f"DEBUG: Converted metadata: {converted_metadata}")
    
    yield {
        'chunk': '',
        'done': True,
        'grounding_metadata': converted_metadata
    }

# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰è¨±å¯ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼
ALLOWED_EXTENSIONS = {'txt', 'pdf', 'docx', 'md', 'doc', 'rtf', 'pptx', 'ppt'}
UPLOAD_FOLDER = 'uploads'
MAX_CONTENT_LENGTH = 16 * 1024 * 1024  # 16MB

# ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆ
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

app.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH

def allowed_file(filename):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‹ãƒã‚§ãƒƒã‚¯"""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def upload_to_cloud_storage(file_path, filename):
    """Cloud Storageã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
    try:
        # Cloud Storageã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        client = storage.Client(project=PROJECT_ID)
        bucket_name = f"{PROJECT_ID}-rag-documents"
        
        # ãƒã‚±ãƒƒãƒˆãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        try:
            bucket = client.get_bucket(bucket_name)
        except Exception:
            bucket = client.create_bucket(bucket_name)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        blob_name = f"documents/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{filename}"
        blob = bucket.blob(blob_name)
        
        with open(file_path, 'rb') as file_data:
            blob.upload_from_file(file_data)
        
        return f"gs://{bucket_name}/{blob_name}"
    except Exception as e:
        print(f"Cloud Storageã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def get_rag_documents():
    """Vertex AI RAGã‹ã‚‰ç›´æ¥ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§ã‚’å–å¾—"""
    try:
        print("RAGã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚Cloud Storageã‹ã‚‰å–å¾—ã—ã¾ã™ã€‚")
        return []
        
        # RAGã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ©Ÿèƒ½ã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
        # client = RagDataServiceClient()
        # ... ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ä¸€æ™‚çš„ã«ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
        
    except Exception as e:
        print(f"RAGãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def get_file_size_string(size_bytes):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’äººé–“ãŒèª­ã¿ã‚„ã™ã„å½¢å¼ã«å¤‰æ›"""
    if size_bytes == 0:
        return "0 B"
    
    size_names = ["B", "KB", "MB", "GB", "TB"]
    i = 0
    while size_bytes >= 1024 and i < len(size_names) - 1:
        size_bytes /= 1024.0
        i += 1
    
    return f"{size_bytes:.1f} {size_names[i]}"

def add_document_to_rag(file_uri, display_name):
    """Vertex AI RAGã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ """
    try:
        print(f"RAGã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã¨ã—ã¦æˆåŠŸã‚’è¿”ã—ã¾ã™: {display_name}")
        
        # RAGã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ©Ÿèƒ½ã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
        # client = RagDataServiceClient()
        # ... ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ä¸€æ™‚çš„ã«ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
        
        return {
            "success": True,
            "operation_name": "placeholder_operation",
            "display_name": display_name,
            "file_uri": file_uri,
            "message": "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’Cloud Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚ï¼ˆRAGæ©Ÿèƒ½ã¯ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ï¼‰"
        }
        
    except Exception as e:
        print(f"RAGãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            "success": False,
            "error": str(e)
        }

def delete_rag_document(rag_file_name):
    """Vertex AI RAGã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤"""
    try:
        print(f"RAGã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã¨ã—ã¦æˆåŠŸã‚’è¿”ã—ã¾ã™: {rag_file_name}")
        
        # RAGã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ©Ÿèƒ½ã‚’ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
        # client = RagDataServiceClient()
        # ... ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ä¸€æ™‚çš„ã«ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
        
        return {
            "success": True,
            "message": "RAGæ©Ÿèƒ½ã¯ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™"
        }
        
    except Exception as e:
        print(f"RAGãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            "success": False,
            "error": str(e)
        }

@app.route('/upload_documents', methods=['POST'])
@auth.login_required
def upload_documents():
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆRAGçµ±åˆç‰ˆï¼‰"""
    try:
        files = request.files.getlist('files')
        
        # ãƒ•ã‚©ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰RAGã‚³ãƒ¼ãƒ‘ã‚¹æƒ…å ±ã‚’å–å¾—
        corpus_name = request.form.get('corpus_name', '').strip()
        corpus_description = request.form.get('corpus_description', '').strip()
        chunk_size = int(request.form.get('chunk_size', 512))
        chunk_overlap = int(request.form.get('chunk_overlap', 100))
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚³ãƒ¼ãƒ‘ã‚¹åã®è¨­å®š
        if not corpus_name:
            corpus_name = f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        if not files:
            return jsonify({'error': 'ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“'}), 400
        
        print(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–‹å§‹:")
        print(f"  ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)}")
        print(f"  RAGã‚³ãƒ¼ãƒ‘ã‚¹å: {corpus_name}")
        print(f"  ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {chunk_size}")
        print(f"  ãƒãƒ£ãƒ³ã‚¯ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—: {chunk_overlap}")
        
        results = []
        
        for file in files:
            if file.filename == '':
                continue
            
            if file and allowed_file(file.filename):
                try:
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å®‰å…¨ã«ã™ã‚‹
                    filename = secure_filename(file.filename)
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    unique_filename = f"{timestamp}_{filename}"
                    
                    # ä¸€æ™‚çš„ã«ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜
                    file_path = os.path.join(UPLOAD_FOLDER, unique_filename)
                    file.save(file_path)
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’å–å¾—
                    file_size = os.path.getsize(file_path)
                    file_size_str = get_file_size_string(file_size)
                    
                    # Cloud Storageã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¨RAGã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ä¸€æ‹¬å®Ÿè¡Œ
                    rag_result = upload_and_import_to_rag(
                        file_path, 
                        unique_filename, 
                        corpus_name, 
                        corpus_description
                    )
                    
                    if rag_result['success']:
                        results.append({
                            'success': True,
                            'filename': filename,
                            'message': f'å®Œäº† ({file_size_str}) - {rag_result.get("message", "")}',
                            'size': file_size_str,
                            'gcs_uri': rag_result.get('gcs_uri'),
                            'corpus_id': rag_result.get('corpus_id'),
                            'corpus_name': rag_result.get('corpus_name'),
                            'corpus_created': rag_result.get('corpus_created', False)
                        })
                    else:
                        results.append({
                            'success': False,
                            'filename': filename,
                            'message': f'ã‚¨ãƒ©ãƒ¼ ({file_size_str}) - {rag_result.get("message", "")}',
                            'size': file_size_str,
                            'error': rag_result.get('error')
                        })
                    
                    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                    try:
                        os.remove(file_path)
                    except:
                        pass
                    
                except Exception as e:
                    results.append({
                        'success': False,
                        'filename': file.filename,
                        'message': f'å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}',
                        'size': 'unknown'
                    })
            else:
                results.append({
                    'success': False,
                    'filename': file.filename,
                    'message': f'ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚è¨±å¯å½¢å¼: {", ".join(ALLOWED_EXTENSIONS)}',
                    'size': 'unknown'
                })
        
        # çµæœã®çµ±è¨ˆã‚’ä½œæˆ
        total_files = len(results)
        success_files = len([r for r in results if r['success']])
        error_files = total_files - success_files
        
        # ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆæƒ…å ±ã‚’å–å¾—
        corpus_created = any(r.get('corpus_created', False) for r in results if r['success'])
        corpus_ids = list(set(r.get('corpus_id') for r in results if r['success'] and r.get('corpus_id')))
        
        return jsonify({
            'results': results,
            'summary': {
                'total_files': total_files,
                'success_files': success_files,
                'error_files': error_files,
                'corpus_name': corpus_name,
                'corpus_created': corpus_created,
                'corpus_ids': corpus_ids
            }
        })
    
    except Exception as e:
        print(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

@app.route('/get_documents', methods=['GET'])
@auth.login_required
def get_documents():
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§å–å¾—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        use_rag_api = request.args.get('use_rag_api', 'false').lower() == 'true'
        
        if use_rag_api:
            # Vertex AI RAGã®APIã‚’ç›´æ¥ä½¿ç”¨
            documents = get_rag_documents()
            return jsonify({'documents': documents, 'source': 'rag_api'})
        else:
            # Cloud Storageã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§ã‚’å–å¾—ï¼ˆæ—¢å­˜ã®å®Ÿè£…ï¼‰
            client = storage.Client(project=PROJECT_ID)
            bucket_name = f"{PROJECT_ID}-rag-documents"
            
            try:
                bucket = client.get_bucket(bucket_name)
                blobs = bucket.list_blobs(prefix="documents/")
                
                documents = []
                for blob in blobs:
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŠ½å‡º
                    filename = blob.name.split('/')[-1]
                    original_filename = '_'.join(filename.split('_')[2:]) if '_' in filename else filename
                    
                    documents.append({
                        'id': blob.name,
                        'title': original_filename,
                        'created_at': blob.time_created.strftime('%Y-%m-%d %H:%M:%S'),
                        'size': get_file_size_string(blob.size),
                        'gcs_uri': f"gs://{bucket_name}/{blob.name}",
                        'source': 'cloud_storage'
                    })
                
                # ä½œæˆæ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
                documents.sort(key=lambda x: x['created_at'], reverse=True)
                
                return jsonify({'documents': documents, 'source': 'cloud_storage'})
                
            except Exception as e:
                print(f"ãƒã‚±ãƒƒãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                return jsonify({'documents': [], 'source': 'cloud_storage'})
    
    except Exception as e:
        print(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'error': f'ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

@app.route('/delete_document/<path:doc_id>', methods=['DELETE'])
@auth.login_required
def delete_document(doc_id):
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‰Šé™¤ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        use_rag_api = request.args.get('use_rag_api', 'false').lower() == 'true'
        
        if use_rag_api:
            # Vertex AI RAGã‹ã‚‰ç›´æ¥å‰Šé™¤
            result = delete_rag_document(doc_id)
            return jsonify(result)
        else:
            # Cloud Storageã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼ˆæ—¢å­˜ã®å®Ÿè£…ï¼‰
            client = storage.Client(project=PROJECT_ID)
            bucket_name = f"{PROJECT_ID}-rag-documents"
            
            try:
                bucket = client.get_bucket(bucket_name)
                blob = bucket.blob(doc_id)
                blob.delete()
                
                return jsonify({
                    'success': True,
                    'message': 'ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤ã—ã¾ã—ãŸï¼ˆCloud Storageã‹ã‚‰ï¼‰'
                })
                
            except Exception as e:
                print(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
                return jsonify({
                    'success': False,
                    'message': f'å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}'
                }), 500
    
    except Exception as e:
        print(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'message': f'å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}'
        }), 500

@app.route('/get_settings', methods=['GET'])
@auth.login_required
def get_settings():
    """è¨­å®šæƒ…å ±å–å¾—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        return jsonify({
            'rag_corpus': RAG_CORPUS,
            'gemini_model': GEMINI_MODEL,
            'project_id': PROJECT_ID,
            'last_update': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
    except Exception as e:
        print(f"è¨­å®šå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({'error': f'è¨­å®šå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

@app.route('/')
@auth.login_required
def index():
    """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸"""
    return render_template('index.html')

@app.route('/health')
def health():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    import psutil
    import gc
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—
    process = psutil.Process()
    memory_info = process.memory_info()
    
    # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
    gc.collect()
    
    return jsonify({
        'status': 'healthy',
        'memory_usage_mb': round(memory_info.rss / 1024 / 1024, 2),
        'memory_percent': round(process.memory_percent(), 2),
        'cpu_percent': round(process.cpu_percent(), 2)
    })

@app.route('/chat', methods=['POST'])
@auth.login_required
def chat():
    """ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    data = request.json
    user_message = data.get('message', '')
    use_deep_mode = data.get('deep_mode', False)
    
    if not user_message:
        return jsonify({'error': 'ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç©ºã§ã™'}), 400
    
    def generate():
        try:
            if use_deep_mode:
                # æ·±æ˜ã‚Šãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
                for chunk_data in generate_deep_response(user_message):
                    yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
            else:
                # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
                for chunk_data in generate_response(user_message):
                    yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            gc.collect()
            
        except Exception as e:
            print(f"Error in chat endpoint: {e}")
            error_data = {
                'chunk': f'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}',
                'done': True,
                'grounding_metadata': None
            }
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            gc.collect()
    
    return Response(generate(), mimetype='text/plain')

# Google DriveåŒæœŸé–¢é€£ã®ãƒ«ãƒ¼ãƒˆ
@app.route('/auth/google')
def google_auth():
    """Google OAuthèªè¨¼ã‚’é–‹å§‹"""
    drive_sync = get_drive_sync()
    if not drive_sync:
        return jsonify({'error': 'Google DriveåŒæœŸæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™'}), 500
    
    redirect_uri = url_for('google_callback', _external=True)
    auth_url, state = drive_sync.get_auth_url(redirect_uri)
    
    session['oauth_state'] = state
    return redirect(auth_url)

@app.route('/auth/google/callback')
def google_callback():
    """Google OAuthèªè¨¼ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    print(f"DEBUG: Callback received - URL: {request.url}")
    print(f"DEBUG: Session oauth_state: {session.get('oauth_state')}")
    
    drive_sync = get_drive_sync()
    if not drive_sync:
        print("ERROR: Google DriveåŒæœŸæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™")
        return jsonify({'error': 'Google DriveåŒæœŸæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™'}), 500
    
    state = session.get('oauth_state')
    if not state:
        print("ERROR: èªè¨¼çŠ¶æ…‹ãŒç„¡åŠ¹ã§ã™")
        return jsonify({'error': 'èªè¨¼çŠ¶æ…‹ãŒç„¡åŠ¹ã§ã™'}), 400
    
    redirect_uri = url_for('google_callback', _external=True)
    authorization_response = request.url
    
    print(f"DEBUG: Calling handle_oauth_callback")
    
    if drive_sync.handle_oauth_callback(authorization_response, state, redirect_uri):
        print("SUCCESS: OAuthèªè¨¼æˆåŠŸ")
        return redirect(url_for('index') + '?auth=success')
    else:
        print("ERROR: OAuthèªè¨¼å¤±æ•—")
        return redirect(url_for('index') + '?auth=error')

@app.route('/drive/status')
def drive_status():
    """Google DriveåŒæœŸçŠ¶æ…‹ã‚’å–å¾—"""
    drive_sync = get_drive_sync()
    if not drive_sync:
        return jsonify({'error': 'Google DriveåŒæœŸæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™'}), 500
    
    # ä¿å­˜ã•ã‚ŒãŸèªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã¿
    drive_sync.load_credentials()
    
    status = drive_sync.get_sync_status()
    return jsonify(status)

@app.route('/drive/sync', methods=['POST'])
@auth.login_required
def sync_drive():
    """Google Driveãƒ•ã‚©ãƒ«ãƒ€ã‚’åŒæœŸ"""
    drive_sync = get_drive_sync()
    if not drive_sync:
        return jsonify({'error': 'Google DriveåŒæœŸæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™'}), 500
    
    # ä¿å­˜ã•ã‚ŒãŸèªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã¿
    if not drive_sync.load_credentials():
        return jsonify({'error': 'èªè¨¼ãŒå¿…è¦ã§ã™', 'auth_required': True}), 401
    
    data = request.get_json() or {}
    folder_name = data.get('folder_name', DRIVE_FOLDER_NAME)
    corpus_name = data.get('corpus_name', 'Drive Sync Corpus')
    force_sync = data.get('force_sync', False)
    recursive = data.get('recursive_sync', False)  # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰é€ä¿¡ã•ã‚Œã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åã«åˆã‚ã›ã¦ä¿®æ­£
    try:
        max_depth = int(data.get('max_depth', 3))      # æ–‡å­—åˆ—ã‹ã‚‰æ•´æ•°ã«å¤‰æ›
        if max_depth < 1 or max_depth > 10:
            max_depth = 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã«æˆ»ã™
    except (ValueError, TypeError):
        max_depth = 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    try:
        result = drive_sync.sync_folder_to_rag(
            folder_name, 
            corpus_name, 
            force_sync, 
            recursive, 
            max_depth
        )
        return jsonify(result)
    except Exception as e:
        return jsonify({'error': f'åŒæœŸã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

@app.route('/drive/folders')
def list_drive_folders():
    """Google Driveã®ãƒ•ã‚©ãƒ«ãƒ€ä¸€è¦§ã‚’å–å¾—"""
    drive_sync = get_drive_sync()
    if not drive_sync:
        return jsonify({'error': 'Google DriveåŒæœŸæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™'}), 500
    
    if not drive_sync.load_credentials():
        return jsonify({'error': 'èªè¨¼ãŒå¿…è¦ã§ã™', 'auth_required': True}), 401
    
    try:
        parent_id = request.args.get('parent', 'root')
        folders = drive_sync.list_folders(parent_id)
        
        # ãƒ•ã‚©ãƒ«ãƒ€æƒ…å ±ã«ãƒ‘ã‚¹æƒ…å ±ã‚’è¿½åŠ 
        folder_list = []
        for folder in folders:
            folder_path = drive_sync.get_folder_path(folder['id'])
            folder_list.append({
                'id': folder['id'],
                'name': folder['name'],
                'path': folder_path,
                'created_time': folder.get('createdTime'),
                'modified_time': folder.get('modifiedTime')
            })
        
        return jsonify({
            'folders': folder_list,
            'parent_id': parent_id
        })
        
    except Exception as e:
        return jsonify({'error': f'ãƒ•ã‚©ãƒ«ãƒ€ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

@app.route('/drive/clear_auth', methods=['POST'])
@auth.login_required
def clear_drive_auth():
    """Google Driveèªè¨¼ã‚’ã‚¯ãƒªã‚¢"""
    drive_sync = get_drive_sync()
    if not drive_sync:
        return jsonify({'error': 'Google DriveåŒæœŸæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™'}), 500
    
    drive_sync.clear_auth()
    return jsonify({'message': 'èªè¨¼æƒ…å ±ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ'})

@app.route('/drive/folder/info', methods=['POST'])
def get_folder_info():
    """ãƒ•ã‚©ãƒ«ãƒ€æƒ…å ±ã‚’å–å¾—ï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ï¼‰"""
    drive_sync = get_drive_sync()
    if not drive_sync:
        return jsonify({'error': 'Google DriveåŒæœŸæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™'}), 500
    
    if not drive_sync.load_credentials():
        return jsonify({'error': 'èªè¨¼ãŒå¿…è¦ã§ã™', 'auth_required': True}), 401
    
    data = request.get_json() or {}
    folder_input = data.get('folder_input', '').strip()
    
    if not folder_input:
        return jsonify({'error': 'ãƒ•ã‚©ãƒ«ãƒ€æŒ‡å®šãŒå¿…è¦ã§ã™'}), 400
    
    try:
        folder_id = drive_sync.find_folder_by_name(folder_input)
        
        if not folder_id:
            return jsonify({
                'success': False,
                'error': 'ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“',
                'input': folder_input
            })
        
        # ãƒ•ã‚©ãƒ«ãƒ€è©³ç´°æƒ…å ±ã‚’å–å¾—
        folder_info = drive_sync._call_drive_files_get(
            folder_id,
            "id, name, createdTime, modifiedTime, webViewLink, parents"
        )
        
        if not folder_info:
            return jsonify({
                'success': False,
                'error': 'ãƒ•ã‚©ãƒ«ãƒ€æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ',
                'input': folder_input
            }), 500
        
        # ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’å–å¾—
        recursive_preview = data.get('recursive_preview', False)
        try:
            max_depth = int(data.get('max_depth', 3))  # æ–‡å­—åˆ—ã‹ã‚‰æ•´æ•°ã«å¤‰æ›
            if max_depth < 1 or max_depth > 10:
                max_depth = 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã«æˆ»ã™
        except (ValueError, TypeError):
            max_depth = 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        
        if recursive_preview:
            # å†å¸°çš„ã«ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’å–å¾—
            print(f"DEBUG: å†å¸°çš„æ¢ç´¢ã‚’é–‹å§‹ - folder_id: {folder_id}, max_depth: {max_depth}")
            
            # æ—¢å­˜ã®å†å¸°æ¢ç´¢çµæœã‚’æ´»ç”¨ã—ã¦å…¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’è¨ˆç®—
            files = drive_sync.list_folder_files_recursive(folder_id, max_depth)
            
            # å…¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’åŠ¹ç‡çš„ã«è¨ˆç®—ï¼ˆãƒ­ã‚°ã‹ã‚‰æ¨å®šï¼‰
            # list_folder_files_recursive ã¯æ—¢ã«å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢ã—ã¦ã„ã‚‹ã®ã§ã€
            # ãƒ­ã‚°ã«å‡ºåŠ›ã•ã‚ŒãŸæƒ…å ±ã‚’åˆ©ç”¨
            # ã‚µãƒãƒ¼ãƒˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ç´„2-5å€ç¨‹åº¦ãŒå…¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã®æ¨å®šå€¤
            estimated_multiplier = 3  # ã‚µãƒãƒ¼ãƒˆå¯¾è±¡å¤–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è€ƒæ…®ã—ãŸæ¨å®šå€ç‡
            total_file_count = len(files) * estimated_multiplier if len(files) > 0 else 0
            
            print(f"DEBUG: å†å¸°çš„æ¢ç´¢çµæœ - ã‚µãƒãƒ¼ãƒˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)} (æ¨å®šç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_file_count})")
        else:
            # ç›´ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å–å¾—
            print(f"DEBUG: ç›´ä¸‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å–å¾— - folder_id: {folder_id}")
            files = drive_sync.list_folder_files(folder_id)
            total_file_count = len(files)  # ç›´ä¸‹ã®å ´åˆã¯åŒã˜
            print(f"DEBUG: ç›´ä¸‹ãƒ•ã‚¡ã‚¤ãƒ«çµæœ - ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files)}")
        
        # ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’å–å¾—
        folder_path = drive_sync.get_folder_path(folder_id)
        print(f"DEBUG: ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹: {folder_path}")
        
        # ã‚µãƒãƒ¼ãƒˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’è¨ˆç®—
        supported_mimetypes = [
            'application/pdf', 'text/plain', 
            'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            'application/msword', 'text/markdown', 'application/rtf',
            'application/vnd.google-apps.document',
            'application/vnd.openxmlformats-officedocument.presentationml.presentation',
            'application/vnd.ms-powerpoint',
            'application/vnd.google-apps.presentation'
        ]
        
        supported_files_count = len([f for f in files if f.get('mimeType') in supported_mimetypes])
        print(f"DEBUG: ã‚µãƒãƒ¼ãƒˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {supported_files_count} / {total_file_count}")
        
        # å†å¸°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯æ·±åº¦åˆ¥çµ±è¨ˆã‚‚è¿½åŠ 
        depth_stats = {}
        if recursive_preview:
            for file in files:
                depth = file.get('depth', 1)
                depth_stats[depth] = depth_stats.get(depth, 0) + 1
            print(f"DEBUG: æ·±åº¦åˆ¥çµ±è¨ˆ: {depth_stats}")
        
        print(f"DEBUG: è¿”ã™ãƒ•ã‚©ãƒ«ãƒ€æƒ…å ± - name: {folder_info['name']}, total_file_count: {total_file_count}, supported_files: {supported_files_count}")
        
        return jsonify({
            'success': True,
            'folder': {
                'id': folder_info['id'],
                'name': folder_info['name'],
                'path': folder_path,
                'created_time': folder_info.get('createdTime'),
                'modified_time': folder_info.get('modifiedTime'),
                'web_view_link': folder_info.get('webViewLink'),
                'file_count': total_file_count,  # å…¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°
                'supported_files': supported_files_count,  # ã‚µãƒãƒ¼ãƒˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°
                'recursive_mode': recursive_preview,
                'max_depth': max_depth if recursive_preview else 1,
                'depth_stats': depth_stats if recursive_preview else {}
            },
            'input': folder_input
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ãƒ•ã‚©ãƒ«ãƒ€æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}',
            'input': folder_input
        }), 500

@app.route('/drive/debug_folder_contents', methods=['POST'])
@auth.login_required
def debug_folder_contents():
    """ãƒ•ã‚©ãƒ«ãƒ€å†…å®¹ã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å–å¾—"""
    drive_sync = get_drive_sync()
    if not drive_sync:
        return jsonify({'error': 'Google DriveåŒæœŸæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™'}), 500
    
    if not drive_sync.load_credentials():
        return jsonify({'error': 'èªè¨¼ãŒå¿…è¦ã§ã™', 'auth_required': True}), 401
    
    data = request.get_json() or {}
    folder_id = data.get('folder_id', '').strip()
    
    if not folder_id:
        return jsonify({'error': 'ãƒ•ã‚©ãƒ«ãƒ€IDãŒå¿…è¦ã§ã™'}), 400
    
    try:
        # ãƒ•ã‚©ãƒ«ãƒ€åŸºæœ¬æƒ…å ±ã‚’å–å¾—
        folder_info = drive_sync._call_drive_files_get(folder_id, "id, name, mimeType, driveId, parents")
        if not folder_info:
            return jsonify({'error': 'ãƒ•ã‚©ãƒ«ãƒ€æƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“'}), 404
        
        # å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–IDã‚’å–å¾—
        drive_id = drive_sync._get_drive_id_for_folder(folder_id)
        
        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆåˆ¶é™ãªã—ï¼‰ã‚’å–å¾—
        all_files_query = f"parents in '{folder_id}' and trashed=false"
        all_files = drive_sync._call_drive_files_list(
            all_files_query,
            "files(id, name, mimeType, size, modifiedTime)",
            page_size=100,
            drive_id=drive_id
        )
        
        # ã‚µãƒãƒ¼ãƒˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å–å¾—
        supported_mimetypes = [
            'application/pdf',
            'text/plain',
            'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            'application/msword',
            'text/markdown',
            'application/rtf',
            'application/vnd.google-apps.document',
            'application/vnd.openxmlformats-officedocument.presentationml.presentation',
            'application/vnd.ms-powerpoint'
        ]
        mimetype_query = " or ".join([f"mimeType='{mt}'" for mt in supported_mimetypes])
        supported_files_query = f"parents in '{folder_id}' and ({mimetype_query}) and trashed=false"
        supported_files = drive_sync._call_drive_files_list(
            supported_files_query,
            "files(id, name, mimeType, size, modifiedTime)",
            page_size=100,
            drive_id=drive_id
        )
        
        return jsonify({
            'success': True,
            'folder_info': folder_info,
            'drive_id': drive_id,
            'is_shared_drive': drive_id is not None,
            'all_files_count': len(all_files),
            'all_files': all_files,
            'supported_files_count': len(supported_files),
            'supported_files': supported_files,
            'queries': {
                'all_files': all_files_query,
                'supported_files': supported_files_query
            }
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ãƒ‡ãƒãƒƒã‚°æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}',
            'folder_id': folder_id
        }), 500

@app.route('/drive/test_shared_access', methods=['POST'])
@auth.login_required
def test_shared_drive_access():
    """å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ"""
    drive_sync = get_drive_sync()
    if not drive_sync:
        return jsonify({'error': 'Google DriveåŒæœŸæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™'}), 500
    
    if not drive_sync.load_credentials():
        return jsonify({'error': 'èªè¨¼ãŒå¿…è¦ã§ã™', 'auth_required': True}), 401
    
    data = request.get_json() or {}
    folder_input = data.get('folder_input', '').strip()
    
    if not folder_input:
        return jsonify({'error': 'ãƒ•ã‚©ãƒ«ãƒ€æŒ‡å®šãŒå¿…è¦ã§ã™'}), 400
    
    try:
        print(f"=== å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
        print(f"å…¥åŠ›: {folder_input}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: èªè¨¼æƒ…å ±ã¨ã‚¹ã‚³ãƒ¼ãƒ—ã®ç¢ºèª
        test_results = {
            'input': folder_input,
            'steps': [],
            'final_result': None,
            'errors': []
        }
        
        # èªè¨¼ã‚¹ã‚³ãƒ¼ãƒ—ã®ç¢ºèª
        scopes = drive_sync.credentials.scopes if drive_sync.credentials else []
        test_results['steps'].append({
            'step': 'èªè¨¼ã‚¹ã‚³ãƒ¼ãƒ—ç¢ºèª',
            'status': 'success',
            'data': {'scopes': scopes}
        })
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—
        try:
            user_info = drive_sync.get_user_info()
            test_results['steps'].append({
                'step': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—',
                'status': 'success',
                'data': user_info
            })
        except Exception as e:
            test_results['steps'].append({
                'step': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾—',
                'status': 'error',
                'error': str(e)
            })
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ•ã‚©ãƒ«ãƒ€æ¤œç´¢ãƒ†ã‚¹ãƒˆ
        try:
            folder_id = drive_sync.find_folder_by_name(folder_input)
            test_results['steps'].append({
                'step': 'ãƒ•ã‚©ãƒ«ãƒ€æ¤œç´¢',
                'status': 'success' if folder_id else 'not_found',
                'data': {'folder_id': folder_id}
            })
            
            if folder_id:
                # ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ•ã‚©ãƒ«ãƒ€è©³ç´°æƒ…å ±å–å¾—
                try:
                    folder_info = drive_sync._call_drive_files_get(
                        folder_id,
                        "id, name, createdTime, modifiedTime, webViewLink, parents, driveId, capabilities"
                    )
                    test_results['steps'].append({
                        'step': 'ãƒ•ã‚©ãƒ«ãƒ€è©³ç´°å–å¾—',
                        'status': 'success',
                        'data': folder_info
                    })
                    
                    # ã‚¹ãƒ†ãƒƒãƒ—5: å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–æƒ…å ±å–å¾—
                    drive_id = folder_info.get('driveId')
                    if drive_id:
                        try:
                            # å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
                            drive_info = drive_sync.drive_service.drives().get(
                                driveId=drive_id,
                                fields="id, name, capabilities, restrictions"
                            ).execute()
                            test_results['steps'].append({
                                'step': 'å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–è©³ç´°å–å¾—',
                                'status': 'success',
                                'data': drive_info
                            })
                        except Exception as e:
                            test_results['steps'].append({
                                'step': 'å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–è©³ç´°å–å¾—',
                                'status': 'error',
                                'error': str(e)
                            })
                    
                    # ã‚¹ãƒ†ãƒƒãƒ—6: ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ãƒ†ã‚¹ãƒˆ
                    try:
                        files = drive_sync.list_folder_files(folder_id)
                        test_results['steps'].append({
                            'step': 'ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—',
                            'status': 'success',
                            'data': {
                                'file_count': len(files),
                                'files': files[:10]  # æœ€åˆã®10ä»¶ã®ã¿
                            }
                        })
                        
                        test_results['final_result'] = {
                            'success': True,
                            'folder_id': folder_id,
                            'file_count': len(files),
                            'is_shared_drive': drive_id is not None,
                            'drive_id': drive_id
                        }
                        
                    except Exception as e:
                        test_results['steps'].append({
                            'step': 'ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—',
                            'status': 'error',
                            'error': str(e)
                        })
                        test_results['errors'].append(f'ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}')
                    
                except Exception as e:
                    test_results['steps'].append({
                        'step': 'ãƒ•ã‚©ãƒ«ãƒ€è©³ç´°å–å¾—',
                        'status': 'error',
                        'error': str(e)
                    })
                    test_results['errors'].append(f'ãƒ•ã‚©ãƒ«ãƒ€è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}')
            else:
                test_results['final_result'] = {
                    'success': False,
                    'error': 'ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
                }
            
        except Exception as e:
            test_results['steps'].append({
                'step': 'ãƒ•ã‚©ãƒ«ãƒ€æ¤œç´¢',
                'status': 'error',
                'error': str(e)
            })
            test_results['errors'].append(f'ãƒ•ã‚©ãƒ«ãƒ€æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}')
        
        print(f"=== ãƒ†ã‚¹ãƒˆå®Œäº† ===")
        
        return jsonify({
            'success': True,
            'test_results': test_results
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}',
            'input': folder_input
        }), 500

@app.route('/drive/check_permissions', methods=['GET'])
@auth.login_required
def check_drive_permissions():
    """å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–ã®æ¨©é™ã‚’ãƒã‚§ãƒƒã‚¯"""
    drive_sync = get_drive_sync()
    if not drive_sync:
        return jsonify({'error': 'Google DriveåŒæœŸæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™'}), 500
    
    if not drive_sync.load_credentials():
        return jsonify({'error': 'èªè¨¼ãŒå¿…è¦ã§ã™', 'auth_required': True}), 401
    
    try:
        result = drive_sync.check_shared_drive_permissions()
        return jsonify(result)
    except Exception as e:
        return jsonify({'error': f'æ¨©é™ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

@app.route('/drive/force_reauth', methods=['POST'])
@auth.login_required
def force_drive_reauth():
    """Google Driveèªè¨¼ã‚’å¼·åˆ¶çš„ã«ãƒªã‚»ãƒƒãƒˆ"""
    drive_sync = get_drive_sync()
    if not drive_sync:
        return jsonify({'error': 'Google DriveåŒæœŸæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™'}), 500
    
    try:
        drive_sync.force_reauth()
        return jsonify({
            'success': True,
            'message': 'èªè¨¼æƒ…å ±ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸã€‚å†èªè¨¼ãŒå¿…è¦ã§ã™ã€‚',
            'required_scopes': drive_sync.scopes
        })
    except Exception as e:
        return jsonify({'error': f'èªè¨¼ãƒªã‚»ãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

@app.route('/drive/test_detailed_access', methods=['POST'])
@auth.login_required
def test_detailed_drive_access():
    """å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–ã¸ã®è©³ç´°ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ"""
    drive_sync = get_drive_sync()
    if not drive_sync:
        return jsonify({'error': 'Google DriveåŒæœŸæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™'}), 500
    
    if not drive_sync.load_credentials():
        return jsonify({'error': 'èªè¨¼ãŒå¿…è¦ã§ã™', 'auth_required': True}), 401
    
    data = request.get_json() or {}
    folder_input = data.get('folder_input', '').strip()
    
    if not folder_input:
        return jsonify({'error': 'ãƒ•ã‚©ãƒ«ãƒ€æŒ‡å®šãŒå¿…è¦ã§ã™'}), 400
    
    try:
        # ã¾ãšãƒ•ã‚©ãƒ«ãƒ€IDã‚’å–å¾—
        folder_id = drive_sync.find_folder_by_name(folder_input)
        
        if not folder_id:
            return jsonify({
                'success': False,
                'error': 'ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“',
                'input': folder_input
            })
        
        # è©³ç´°ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
        test_results = drive_sync.test_shared_drive_access(folder_id)
        
        return jsonify({
            'success': True,
            'input': folder_input,
            'folder_id': folder_id,
            'test_results': test_results
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'è©³ç´°ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}',
            'input': folder_input
        }), 500

@app.route('/drive/list_shared_drives', methods=['GET'])
@auth.login_required
def list_shared_drives():
    """å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–ä¸€è¦§ã‚’å–å¾—"""
    drive_sync = get_drive_sync()
    if not drive_sync:
        return jsonify({'error': 'Google DriveåŒæœŸæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™'}), 500
    
    if not drive_sync.load_credentials():
        return jsonify({'error': 'èªè¨¼ãŒå¿…è¦ã§ã™', 'auth_required': True}), 401
    
    try:
        drives = drive_sync.list_shared_drives()
        return jsonify({
            'success': True,
            'drives': drives,
            'count': len(drives)
        })
    except Exception as e:
        return jsonify({'error': f'å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

@app.route('/drive/shared_drive_root_files/<drive_id>', methods=['GET'])
@auth.login_required
def get_shared_drive_root_files(drive_id):
    """å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–ã®ãƒ«ãƒ¼ãƒˆç›´ä¸‹ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    drive_sync = get_drive_sync()
    if not drive_sync:
        return jsonify({'error': 'Google DriveåŒæœŸæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™'}), 500
    
    if not drive_sync.load_credentials():
        return jsonify({'error': 'èªè¨¼ãŒå¿…è¦ã§ã™', 'auth_required': True}), 401
    
    try:
        files = drive_sync.get_shared_drive_root_files(drive_id)
        return jsonify({
            'success': True,
            'files': files,
            'count': len(files),
            'drive_id': drive_id
        })
    except Exception as e:
        return jsonify({'error': f'å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

def extract_text_from_pptx(file_path):
    """PowerPointãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    try:
        presentation = Presentation(file_path)
        text_content = []
        
        for slide_num, slide in enumerate(presentation.slides, 1):
            slide_text = f"\n--- ã‚¹ãƒ©ã‚¤ãƒ‰ {slide_num} ---\n"
            
            for shape in slide.shapes:
                if hasattr(shape, "text") and shape.text.strip():
                    slide_text += shape.text.strip() + "\n"
            
            text_content.append(slide_text)
        
        return "\n".join(text_content)
    except Exception as e:
        print(f"PowerPointãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
        return ""

def extract_text_from_ppt(file_path):
    """PowerPoint (.ppt) ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    try:
        # .pptãƒ•ã‚¡ã‚¤ãƒ«ã¯è¤‡é›‘ãªå‡¦ç†ãŒå¿…è¦ãªãŸã‚ã€åŸºæœ¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ã¿
        # å®Ÿç”¨çš„ã«ã¯.pptxã¸ã®å¤‰æ›ã‚’æ¨å¥¨
        print(f"è­¦å‘Š: .pptãƒ•ã‚¡ã‚¤ãƒ«ã¯é™å®šçš„ãªã‚µãƒãƒ¼ãƒˆã§ã™ã€‚.pptxã§ã®ä¿å­˜ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        return f"PowerPoint ãƒ•ã‚¡ã‚¤ãƒ«: {os.path.basename(file_path)}\nï¼ˆãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã«ã¯åˆ¶é™ãŒã‚ã‚Šã¾ã™ï¼‰"
    except Exception as e:
        print(f"PowerPoint (.ppt) å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return ""

@app.route('/drive/folder/files_recursive', methods=['POST'])
@auth.login_required
def get_folder_files_recursive():
    """ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«å–å¾—"""
    drive_sync = get_drive_sync()
    if not drive_sync:
        return jsonify({'error': 'Google DriveåŒæœŸæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™'}), 500
    
    if not drive_sync.load_credentials():
        return jsonify({'error': 'èªè¨¼ãŒå¿…è¦ã§ã™', 'auth_required': True}), 401
    
    data = request.get_json() or {}
    folder_input = data.get('folder_input', '').strip()
    try:
        max_depth = int(data.get('max_depth', 3))  # æ–‡å­—åˆ—ã‹ã‚‰æ•´æ•°ã«å¤‰æ›
        if max_depth < 1 or max_depth > 10:
            max_depth = 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã«æˆ»ã™
    except (ValueError, TypeError):
        max_depth = 3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    if not folder_input:
        return jsonify({'error': 'ãƒ•ã‚©ãƒ«ãƒ€æŒ‡å®šãŒå¿…è¦ã§ã™'}), 400
    
    try:
        # ãƒ•ã‚©ãƒ«ãƒ€IDã‚’å–å¾—
        folder_id = drive_sync.find_folder_by_name(folder_input)
        
        if not folder_id:
            return jsonify({
                'success': False,
                'error': 'ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“',
                'input': folder_input
            })
        
        # å†å¸°çš„ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
        files = drive_sync.list_folder_files_recursive(folder_id, max_depth)
        
        # æ·±åº¦åˆ¥çµ±è¨ˆã‚’ä½œæˆ
        depth_stats = {}
        folder_stats = {}
        
        for file in files:
            depth = file.get('depth', 1)
            folder_path = file.get('folder_path', '/')
            
            depth_stats[depth] = depth_stats.get(depth, 0) + 1
            folder_stats[folder_path] = folder_stats.get(folder_path, 0) + 1
        
        return jsonify({
            'success': True,
            'folder_input': folder_input,
            'folder_id': folder_id,
            'total_files': len(files),
            'max_depth': max_depth,
            'depth_stats': depth_stats,
            'folder_stats': folder_stats,
            'files': files[:50],  # æœ€åˆã®50ä»¶ã®ã¿è¿”ã™ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è€ƒæ…®ï¼‰
            'files_truncated': len(files) > 50
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'å†å¸°çš„ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}',
            'input': folder_input
        }), 500

def get_rag_corpora():
    """åˆ©ç”¨å¯èƒ½ãªRAGã‚³ãƒ¼ãƒ‘ã‚¹ä¸€è¦§ã‚’å–å¾—"""
    try:
        print("RAGã‚³ãƒ¼ãƒ‘ã‚¹ä¸€è¦§å–å¾—ã‚’è©¦è¡Œä¸­...")
        
        # æ–¹æ³•1: å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé€šã‚Šã®vertexai.preview.ragã‚’ä½¿ç”¨
        try:
            import vertexai
            from vertexai.preview import rag
            
            # Vertex AI ã‚’åˆæœŸåŒ–
            vertexai.init(project=PROJECT_ID, location="us-central1")
            print("Vertex AIåˆæœŸåŒ–å®Œäº†")
            
            # RAGã‚³ãƒ¼ãƒ‘ã‚¹ä¸€è¦§ã‚’å–å¾—
            corpora_list = rag.list_corpora()
            print(f"rag.list_corpora()ã®çµæœ: {type(corpora_list)}")
            
            corpora = []
            for corpus in corpora_list:
                print(f"ã‚³ãƒ¼ãƒ‘ã‚¹è©³ç´°: {corpus}")
                corpora.append({
                    'id': corpus.name if hasattr(corpus, 'name') else str(corpus),
                    'display_name': corpus.display_name if hasattr(corpus, 'display_name') else f"ã‚³ãƒ¼ãƒ‘ã‚¹ {corpus.name.split('/')[-1][:8]}..." if hasattr(corpus, 'name') else 'Unknown',
                    'description': corpus.description if hasattr(corpus, 'description') else '',
                    'created_time': corpus.create_time.strftime('%Y-%m-%d %H:%M:%S') if hasattr(corpus, 'create_time') and corpus.create_time else 'N/A'
                })
            
            if corpora:
                print(f"Vertex AI RAG APIçµŒç”±ã§ã‚³ãƒ¼ãƒ‘ã‚¹ {len(corpora)} å€‹ã‚’å–å¾—ã—ã¾ã—ãŸ")
                return corpora
            else:
                print("Vertex AI RAG API: ã‚³ãƒ¼ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                
        except ImportError as e:
            print(f"vertexai.preview.rag ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        except AttributeError as e:
            print(f"vertexai.preview.rag ã‚¢ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        except Exception as e:
            print(f"Vertex AI RAG APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
        
        # æ–¹æ³•2: REST APIã‚’ç›´æ¥ä½¿ç”¨ï¼ˆå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®curlã‚³ãƒãƒ³ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰
        try:
            import requests
            from google.auth import default
            from google.auth.transport.requests import Request as AuthRequest
            
            print("REST APIçµŒç”±ã§RAGã‚³ãƒ¼ãƒ‘ã‚¹ä¸€è¦§ã‚’å–å¾—ä¸­...")
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆèªè¨¼æƒ…å ±ã‚’å–å¾—
            credentials, _ = default()
            credentials.refresh(AuthRequest())
            
            # å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«åŸºã¥ãREST APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
            url = f"https://us-central1-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/us-central1/ragCorpora"
            
            headers = {
                'Authorization': f'Bearer {credentials.token}',
                'Content-Type': 'application/json'
            }
            
            print(f"APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: {url}")
            
            response = requests.get(url, headers=headers, timeout=30)
            print(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                print(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿: {data}")
                
                corpora = []
                
                for corpus in data.get('ragCorpora', []):
                    corpora.append({
                        'id': corpus.get('name', ''),
                        'display_name': corpus.get('displayName', 'Unknown'),
                        'description': corpus.get('description', ''),
                        'created_time': corpus.get('createTime', 'N/A')
                    })
                
                if corpora:
                    print(f"REST APIçµŒç”±ã§RAGã‚³ãƒ¼ãƒ‘ã‚¹ {len(corpora)} å€‹ã‚’å–å¾—ã—ã¾ã—ãŸ")
                    return corpora
                else:
                    print("REST APIçµŒç”±: ã‚³ãƒ¼ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    
            elif response.status_code == 404:
                print("REST API: RAGã‚³ãƒ¼ãƒ‘ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆ404ï¼‰")
            elif response.status_code == 403:
                print("REST API: ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆ403ï¼‰")
            else:
                print(f"REST APIå‘¼ã³å‡ºã—å¤±æ•—: {response.status_code} - {response.text}")
                
        except Exception as e:
            print(f"REST APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
        
        # æ–¹æ³•3: gcloud CLIã‚’ä½¿ç”¨ï¼ˆv1 APIãƒ™ãƒ¼ã‚¹ï¼‰
        try:
            import subprocess
            import json
            
            print("gcloud CLIçµŒç”±ã§RAGã‚³ãƒ¼ãƒ‘ã‚¹ä¸€è¦§ã‚’å–å¾—ä¸­...")
            
            # gcloud aiplatform rag-corpora list ã‚³ãƒãƒ³ãƒ‰
            cmd = [
                'gcloud', 'ai', 'rag-corpora', 'list', 
                f'--project={PROJECT_ID}', 
                '--location=us-central1',
                '--format=json'
            ]
            
            print(f"å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            print(f"gcloudå®Ÿè¡Œçµæœ: returncode={result.returncode}")
            if result.stdout:
                print(f"stdout: {result.stdout}")
            if result.stderr:
                print(f"stderr: {result.stderr}")
            
            if result.returncode == 0 and result.stdout:
                data = json.loads(result.stdout)
                print(f"gcloudçµŒç”±ã§RAGã‚³ãƒ¼ãƒ‘ã‚¹æƒ…å ±ã‚’å–å¾—: {len(data)}ä»¶")
                
                corpora = []
                for corpus in data:
                    corpora.append({
                        'id': corpus.get('name', ''),
                        'display_name': corpus.get('displayName', 'Unknown'),
                        'description': corpus.get('description', ''),
                        'created_time': corpus.get('createTime', 'N/A')
                    })
                
                if corpora:
                    print(f"gcloud CLIçµŒç”±ã§RAGã‚³ãƒ¼ãƒ‘ã‚¹ {len(corpora)} å€‹ã‚’å–å¾—ã—ã¾ã—ãŸ")
                    return corpora
                    
        except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError, json.JSONDecodeError) as e:
            print(f"gcloud CLIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
        except Exception as e:
            print(f"gcloud CLIå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‘ã‚¹æƒ…å ±ã‚’å–å¾—
        print("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç¾åœ¨ã®ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚³ãƒ¼ãƒ‘ã‚¹æƒ…å ±ã‚’å–å¾—")
        
        # RAG_CORPUSã‹ã‚‰ã‚³ãƒ¼ãƒ‘ã‚¹åã‚’æŠ½å‡º
        corpus_id = RAG_CORPUS
        corpus_name = "ç¾åœ¨ä½¿ç”¨ä¸­ã®ã‚³ãƒ¼ãƒ‘ã‚¹"
        
        # ã‚³ãƒ¼ãƒ‘ã‚¹IDã‹ã‚‰ã‚³ãƒ¼ãƒ‘ã‚¹åã‚’æ¨æ¸¬
        if 'ragCorpora/' in corpus_id:
            # projects/PROJECT_ID/locations/LOCATION/ragCorpora/CORPUS_ID ã®å½¢å¼
            parts = corpus_id.split('/')
            if len(parts) >= 6:
                actual_corpus_id = parts[-1]
                corpus_name = f"RAGã‚³ãƒ¼ãƒ‘ã‚¹ ({actual_corpus_id[:8]}...)"
        
        corpora = [{
            'id': corpus_id,
            'display_name': corpus_name,
            'description': f'ç’°å¢ƒå¤‰æ•° RAG_CORPUS ã§è¨­å®šã•ã‚ŒãŸã‚³ãƒ¼ãƒ‘ã‚¹',
            'created_time': 'N/A'
        }]
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¿½åŠ ã®ã‚³ãƒ¼ãƒ‘ã‚¹æƒ…å ±ã‚’èª­ã¿è¾¼ã¿ï¼ˆã‚ã‚Œã°ï¼‰
        try:
            import configparser
            config_file = 'temp/rag_config.ini'
            
            if os.path.exists(config_file):
                config = configparser.ConfigParser()
                config.read(config_file)
                
                if 'RAG' in config and 'recent_corpora' in config['RAG']:
                    recent_corpora_str = config['RAG']['recent_corpora']
                    recent_corpora = json.loads(recent_corpora_str)
                    
                    for corpus_info in recent_corpora:
                        if corpus_info['id'] != corpus_id:  # é‡è¤‡ã‚’é¿ã‘ã‚‹
                            corpora.append(corpus_info)
                            
        except Exception as e:
            print(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ï¼ˆç„¡è¦–ï¼‰: {e}")
        
        print(f"æœ€çµ‚çš„ã«è¿”ã™ã‚³ãƒ¼ãƒ‘ã‚¹æ•°: {len(corpora)}")
        return corpora
        
    except Exception as e:
        print(f"RAGã‚³ãƒ¼ãƒ‘ã‚¹ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç¾åœ¨ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã®ã¿ã‚’è¿”ã™
        return [{
            'id': RAG_CORPUS,
            'display_name': 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚³ãƒ¼ãƒ‘ã‚¹',
            'description': 'ç’°å¢ƒå¤‰æ•°ã§è¨­å®šã•ã‚ŒãŸRAGã‚³ãƒ¼ãƒ‘ã‚¹',
            'created_time': 'N/A'
        }]

@app.route('/get_rag_corpora', methods=['GET'])
@auth.login_required
def get_rag_corpora_endpoint():
    """RAGã‚³ãƒ¼ãƒ‘ã‚¹ä¸€è¦§å–å¾—ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        corpora = get_rag_corpora()
        return jsonify({
            'success': True,
            'corpora': corpora,
            'current_corpus': RAG_CORPUS
        })
    except Exception as e:
        print(f"RAGã‚³ãƒ¼ãƒ‘ã‚¹ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'error': f'ã‚³ãƒ¼ãƒ‘ã‚¹ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}'
        }), 500

@app.route('/set_rag_corpus', methods=['POST'])
@auth.login_required
def set_rag_corpus():
    """RAGã‚³ãƒ¼ãƒ‘ã‚¹ã‚’å‹•çš„ã«è¨­å®š"""
    try:
        data = request.get_json()
        new_corpus_id = data.get('corpus_id')
        
        if not new_corpus_id:
            return jsonify({
                'success': False,
                'error': 'ã‚³ãƒ¼ãƒ‘ã‚¹IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“'
            }), 400
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’æ›´æ–°
        global RAG_CORPUS
        old_corpus_id = RAG_CORPUS
        RAG_CORPUS = new_corpus_id
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆå±¥æ­´ã‚‚å«ã‚€ï¼‰
        try:
            import configparser
            config_file = 'temp/rag_config.ini'
            os.makedirs(os.path.dirname(config_file), exist_ok=True)
            
            config = configparser.ConfigParser()
            if os.path.exists(config_file):
                config.read(config_file)
            
            if 'RAG' not in config:
                config['RAG'] = {}
            
            # ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‘ã‚¹è¨­å®šã‚’ä¿å­˜
            config['RAG']['current_corpus'] = new_corpus_id
            config['RAG']['last_updated'] = datetime.now().isoformat()
            
            # ã‚³ãƒ¼ãƒ‘ã‚¹å±¥æ­´ã‚’ç®¡ç†
            recent_corpora = []
            if 'recent_corpora' in config['RAG']:
                try:
                    recent_corpora = json.loads(config['RAG']['recent_corpora'])
                except json.JSONDecodeError:
                    recent_corpora = []
            
            # æ–°ã—ã„ã‚³ãƒ¼ãƒ‘ã‚¹æƒ…å ±ã‚’ä½œæˆ
            corpus_name = f"RAGã‚³ãƒ¼ãƒ‘ã‚¹ ({new_corpus_id.split('/')[-1][:8]}...)" if 'ragCorpora/' in new_corpus_id else new_corpus_id
            new_corpus_info = {
                'id': new_corpus_id,
                'display_name': corpus_name,
                'description': f'è¨­å®šæ—¥æ™‚: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}',
                'created_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # æ—¢å­˜ã®å±¥æ­´ã‹ã‚‰åŒã˜IDã‚’å‰Šé™¤
            recent_corpora = [c for c in recent_corpora if c['id'] != new_corpus_id]
            
            # æ–°ã—ã„ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’å…ˆé ­ã«è¿½åŠ 
            recent_corpora.insert(0, new_corpus_info)
            
            # æœ€å¤§5ä»¶ã¾ã§ä¿æŒ
            recent_corpora = recent_corpora[:5]
            
            # å±¥æ­´ã‚’ä¿å­˜
            config['RAG']['recent_corpora'] = json.dumps(recent_corpora, ensure_ascii=False)
            
            with open(config_file, 'w', encoding='utf-8') as f:
                config.write(f)
                
            print(f"RAGã‚³ãƒ¼ãƒ‘ã‚¹è¨­å®šã‚’ä¿å­˜: {new_corpus_id}")
                
        except Exception as e:
            print(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        
        return jsonify({
            'success': True,
            'message': f'RAGã‚³ãƒ¼ãƒ‘ã‚¹ã‚’å¤‰æ›´ã—ã¾ã—ãŸ',
            'old_corpus': old_corpus_id,
            'new_corpus': RAG_CORPUS,
            'change_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
        
    except Exception as e:
        print(f"RAGã‚³ãƒ¼ãƒ‘ã‚¹è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'error': f'ã‚³ãƒ¼ãƒ‘ã‚¹è¨­å®šã‚¨ãƒ©ãƒ¼: {str(e)}'
        }), 500

def create_rag_corpus(corpus_name, description=""):
    """æ–°ã—ã„RAGã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½œæˆ"""
    try:
        import vertexai
        from vertexai.preview import rag
        
        # Vertex AI ã‚’åˆæœŸåŒ–
        vertexai.init(project=PROJECT_ID, location="us-central1")
        print(f"æ–°ã—ã„RAGã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½œæˆä¸­: {corpus_name}")
        
        # RAGã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½œæˆ
        corpus = rag.create_corpus(
            display_name=corpus_name,
            description=description or f"Auto-created corpus: {corpus_name}"
        )
        
        print(f"RAGã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆæˆåŠŸ: {corpus.name}")
        return {
            'success': True,
            'corpus_id': corpus.name,
            'display_name': corpus.display_name,
            'description': corpus.description,
            'message': f'æ–°ã—ã„RAGã‚³ãƒ¼ãƒ‘ã‚¹ "{corpus_name}" ã‚’ä½œæˆã—ã¾ã—ãŸ'
        }
        
    except Exception as e:
        print(f"RAGã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e),
            'message': f'RAGã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}'
        }

def import_files_to_rag_corpus(corpus_id, gcs_uris, chunk_size=512, chunk_overlap=100):
    """Cloud Storageãƒ•ã‚¡ã‚¤ãƒ«ã‚’RAGã‚³ãƒ¼ãƒ‘ã‚¹ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
    try:
        import vertexai
        from vertexai.preview import rag
        
        # Vertex AI ã‚’åˆæœŸåŒ–
        vertexai.init(project=PROJECT_ID, location="us-central1")
        print(f"RAGã‚³ãƒ¼ãƒ‘ã‚¹ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸­: {corpus_id}")
        print(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(gcs_uris)}")
        
        # GCS URIã®ãƒªã‚¹ãƒˆã‚’æº–å‚™
        if isinstance(gcs_uris, str):
            gcs_uris = [gcs_uris]
        
        # RAGãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        response = rag.import_files(
            corpus_name=corpus_id,
            paths=gcs_uris,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        print(f"RAGãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆé–‹å§‹: {response}")
        
        return {
            'success': True,
            'operation': str(response),
            'corpus_id': corpus_id,
            'imported_files': len(gcs_uris),
            'files': gcs_uris,
            'message': f'{len(gcs_uris)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’RAGã‚³ãƒ¼ãƒ‘ã‚¹ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ'
        }
        
    except Exception as e:
        print(f"RAGãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e),
            'corpus_id': corpus_id,
            'message': f'RAGãƒ•ã‚¡ã‚¤ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}'
        }

def find_or_create_rag_corpus(corpus_name, description=""):
    """RAGã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æ¤œç´¢ã—ã€å­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ"""
    try:
        # æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‘ã‚¹ä¸€è¦§ã‚’å–å¾—
        corpora = get_rag_corpora()
        
        # åå‰ã§æ—¢å­˜ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æ¤œç´¢
        existing_corpus = None
        for corpus in corpora:
            if corpus.get('display_name') == corpus_name:
                existing_corpus = corpus
                break
        
        if existing_corpus:
            print(f"æ—¢å­˜ã®RAGã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½¿ç”¨: {corpus_name}")
            return {
                'success': True,
                'corpus_id': existing_corpus['id'],
                'display_name': existing_corpus['display_name'],
                'description': existing_corpus['description'],
                'created': False,
                'message': f'æ—¢å­˜ã®RAGã‚³ãƒ¼ãƒ‘ã‚¹ "{corpus_name}" ã‚’ä½¿ç”¨ã—ã¾ã™'
            }
        else:
            print(f"æ–°ã—ã„RAGã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½œæˆ: {corpus_name}")
            result = create_rag_corpus(corpus_name, description)
            if result['success']:
                result['created'] = True
            return result
            
    except Exception as e:
        print(f"RAGã‚³ãƒ¼ãƒ‘ã‚¹æ¤œç´¢ãƒ»ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        return {
            'success': False,
            'error': str(e),
            'message': f'RAGã‚³ãƒ¼ãƒ‘ã‚¹æ¤œç´¢ãƒ»ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}'
        }

def upload_and_import_to_rag(file_path, filename, corpus_name, description=""):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Cloud Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€RAGã‚³ãƒ¼ãƒ‘ã‚¹ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
    try:
        # ã‚¹ãƒ†ãƒƒãƒ—1: Cloud Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        print(f"ã‚¹ãƒ†ãƒƒãƒ—1: Cloud Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­: {filename}")
        gcs_uri = upload_to_cloud_storage(file_path, filename)
        
        if not gcs_uri:
            return {
                'success': False,
                'error': 'Cloud Storageã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ',
                'filename': filename
            }
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: RAGã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æ¤œç´¢ã¾ãŸã¯ä½œæˆ
        print(f"ã‚¹ãƒ†ãƒƒãƒ—2: RAGã‚³ãƒ¼ãƒ‘ã‚¹æ¤œç´¢ãƒ»ä½œæˆä¸­: {corpus_name}")
        corpus_result = find_or_create_rag_corpus(corpus_name, description)
        
        if not corpus_result['success']:
            return {
                'success': False,
                'error': corpus_result.get('error', 'RAGã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆã«å¤±æ•—'),
                'filename': filename,
                'gcs_uri': gcs_uri
            }
        
        corpus_id = corpus_result['corpus_id']
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: RAGã‚³ãƒ¼ãƒ‘ã‚¹ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        print(f"ã‚¹ãƒ†ãƒƒãƒ—3: RAGã‚³ãƒ¼ãƒ‘ã‚¹ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸­: {filename}")
        import_result = import_files_to_rag_corpus(corpus_id, [gcs_uri])
        
        if import_result['success']:
            return {
                'success': True,
                'filename': filename,
                'gcs_uri': gcs_uri,
                'corpus_id': corpus_id,
                'corpus_name': corpus_name,
                'corpus_created': corpus_result.get('created', False),
                'message': f'ãƒ•ã‚¡ã‚¤ãƒ« "{filename}" ã‚’RAGã‚³ãƒ¼ãƒ‘ã‚¹ "{corpus_name}" ã«æ­£å¸¸ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ'
            }
        else:
            return {
                'success': False,
                'error': import_result.get('error', 'RAGã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—'),
                'filename': filename,
                'gcs_uri': gcs_uri,
                'corpus_id': corpus_id,
                'message': f'Cloud Storageã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¯æˆåŠŸã—ã¾ã—ãŸãŒã€RAGã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ'
            }
        
    except Exception as e:
        print(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e),
            'filename': filename,
            'message': f'å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}'
        }

@app.route('/drive/sync_to_rag', methods=['POST'])
@auth.login_required
def sync_drive_to_rag():
    """Google Driveãƒ•ã‚©ãƒ«ãƒ€ã‚’RAGã‚³ãƒ¼ãƒ‘ã‚¹ã«ç›´æ¥åŒæœŸ"""
    drive_sync = get_drive_sync()
    if not drive_sync:
        return jsonify({'error': 'Google DriveåŒæœŸæ©Ÿèƒ½ãŒç„¡åŠ¹ã§ã™'}), 500
    
    # ä¿å­˜ã•ã‚ŒãŸèªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã¿
    if not drive_sync.load_credentials():
        return jsonify({'error': 'èªè¨¼ãŒå¿…è¦ã§ã™', 'auth_required': True}), 401
    
    data = request.get_json() or {}
    folder_name = data.get('folder_name', DRIVE_FOLDER_NAME)
    corpus_name = data.get('corpus_name', 'Drive Sync Corpus')
    corpus_description = data.get('corpus_description', '')
    chunk_size = int(data.get('chunk_size', 512))
    chunk_overlap = int(data.get('chunk_overlap', 100))
    force_sync = data.get('force_sync', False)
    recursive = data.get('recursive_sync', False)
    
    try:
        max_depth = int(data.get('max_depth', 3))
        if max_depth < 1 or max_depth > 10:
            max_depth = 3
    except (ValueError, TypeError):
        max_depth = 3
    
    try:
        print(f"Google Drive -> RAGåŒæœŸé–‹å§‹:")
        print(f"  ãƒ•ã‚©ãƒ«ãƒ€å: {folder_name}")
        print(f"  RAGã‚³ãƒ¼ãƒ‘ã‚¹å: {corpus_name}")
        print(f"  å†å¸°ãƒ¢ãƒ¼ãƒ‰: {recursive} (æ·±åº¦: {max_depth})")
        print(f"  ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {chunk_size}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: Google Driveãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¤œç´¢
        folder_id = drive_sync.find_folder_by_name(folder_name)
        if not folder_id:
            return jsonify({
                'success': False,
                'error': f'ãƒ•ã‚©ãƒ«ãƒ€ "{folder_name}" ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'
            })
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: RAGã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æ¤œç´¢ã¾ãŸã¯ä½œæˆ
        corpus_result = find_or_create_rag_corpus(corpus_name, corpus_description)
        if not corpus_result['success']:
            return jsonify({
                'success': False,
                'error': f'RAGã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆã«å¤±æ•—: {corpus_result.get("error", "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼")}'
            })
        
        corpus_id = corpus_result['corpus_id']
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: DriveåŒæœŸã§Cloud Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        sync_result = drive_sync.sync_folder_to_rag(
            folder_name, 
            corpus_name, 
            force_sync, 
            recursive, 
            max_depth
        )
        
        if not sync_result.get('success', False):
            return jsonify({
                'success': False,
                'error': f'DriveåŒæœŸã«å¤±æ•—: {sync_result.get("error", "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼")}'
            })
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: Cloud Storageã‹ã‚‰RAGã‚³ãƒ¼ãƒ‘ã‚¹ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        uploaded_files = sync_result.get('uploaded_files', [])
        gcs_uris = [file_info.get('gcs_uri') for file_info in uploaded_files if file_info.get('gcs_uri')]
        
        if gcs_uris:
            print(f"RAGã‚³ãƒ¼ãƒ‘ã‚¹ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ: {len(gcs_uris)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")
            import_result = import_files_to_rag_corpus(
                corpus_id, 
                gcs_uris, 
                chunk_size, 
                chunk_overlap
            )
            
            if import_result['success']:
                return jsonify({
                    'success': True,
                    'message': f'Google Driveã‹ã‚‰{len(gcs_uris)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’RAGã‚³ãƒ¼ãƒ‘ã‚¹ "{corpus_name}" ã«åŒæœŸã—ã¾ã—ãŸ',
                    'sync_result': sync_result,
                    'import_result': import_result,
                    'corpus_id': corpus_id,
                    'corpus_name': corpus_name,
                    'corpus_created': corpus_result.get('created', False),
                    'imported_files': len(gcs_uris)
                })
            else:
                return jsonify({
                    'success': False,
                    'error': f'RAGã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {import_result.get("error", "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼")}',
                    'sync_result': sync_result,
                    'partial_success': True,
                    'uploaded_files': len(gcs_uris)
                })
        else:
            return jsonify({
                'success': True,
                'message': 'Google DriveåŒæœŸã¯å®Œäº†ã—ã¾ã—ãŸãŒã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ',
                'sync_result': sync_result,
                'corpus_id': corpus_id,
                'corpus_name': corpus_name,
                'imported_files': 0
            })
        
    except Exception as e:
        print(f"Drive -> RAGåŒæœŸã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': f'åŒæœŸã‚¨ãƒ©ãƒ¼: {str(e)}'}), 500

@app.route('/create_rag_corpus', methods=['POST'])
@auth.login_required
def create_rag_corpus_endpoint():
    """æ–°ã—ã„RAGã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        data = request.get_json()
        corpus_name = data.get('corpus_name', '').strip()
        description = data.get('description', '').strip()
        
        if not corpus_name:
            return jsonify({
                'success': False,
                'error': 'ã‚³ãƒ¼ãƒ‘ã‚¹åãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“'
            }), 400
        
        # æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‘ã‚¹åã‚’ãƒã‚§ãƒƒã‚¯
        existing_corpora = get_rag_corpora()
        for corpus in existing_corpora:
            if corpus.get('display_name') == corpus_name:
                return jsonify({
                    'success': False,
                    'error': f'ã‚³ãƒ¼ãƒ‘ã‚¹å "{corpus_name}" ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™'
                }), 400
        
        # æ–°ã—ã„ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½œæˆ
        result = create_rag_corpus(corpus_name, description)
        
        if result['success']:
            return jsonify({
                'success': True,
                'corpus_id': result['corpus_id'],
                'display_name': result['display_name'],
                'description': result['description'],
                'message': result['message']
            })
        else:
            return jsonify({
                'success': False,
                'error': result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼'),
                'message': result.get('message', 'ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ')
            }), 500
        
    except Exception as e:
        print(f"RAGã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'error': f'ã‚³ãƒ¼ãƒ‘ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}'
        }), 500

@app.route('/import_to_rag_corpus', methods=['POST'])
@auth.login_required
def import_to_rag_corpus_endpoint():
    """æ—¢å­˜ã®Cloud Storageãƒ•ã‚¡ã‚¤ãƒ«ã‚’RAGã‚³ãƒ¼ãƒ‘ã‚¹ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
    try:
        data = request.get_json()
        corpus_id = data.get('corpus_id', '').strip()
        gcs_uris = data.get('gcs_uris', [])
        chunk_size = int(data.get('chunk_size', 512))
        chunk_overlap = int(data.get('chunk_overlap', 100))
        
        if not corpus_id:
            return jsonify({
                'success': False,
                'error': 'ã‚³ãƒ¼ãƒ‘ã‚¹IDãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“'
            }), 400
        
        if not gcs_uris:
            return jsonify({
                'success': False,
                'error': 'ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“'
            }), 400
        
        # RAGã‚³ãƒ¼ãƒ‘ã‚¹ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        result = import_files_to_rag_corpus(corpus_id, gcs_uris, chunk_size, chunk_overlap)
        
        if result['success']:
            return jsonify({
                'success': True,
                'corpus_id': result['corpus_id'],
                'imported_files': result['imported_files'],
                'files': result['files'],
                'message': result['message']
            })
        else:
            return jsonify({
                'success': False,
                'error': result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼'),
                'message': result.get('message', 'ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ')
            }), 500
        
    except Exception as e:
        print(f"RAGã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return jsonify({
            'success': False,
            'error': f'ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}'
        }), 500

if __name__ == '__main__':
    import sys
    # ãƒãƒ¼ãƒˆç•ªå·ã‚’ç’°å¢ƒå¤‰æ•°ã¾ãŸã¯ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰å–å¾—
    port = int(os.environ.get('PORT', 8080))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’8080ã«å¤‰æ›´
    if len(sys.argv) > 1:
        try:
            port = int(sys.argv[1])
        except ValueError:
            pass
    
    print(f"Starting server on port {port}")
    app.run(host='0.0.0.0', port=port, debug=True) 