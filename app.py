from flask import Flask, request, jsonify, render_template, Response
from flask_httpauth import HTTPBasicAuth
from google import genai
from google.genai import types
import json
import os
import re
from datetime import datetime
import hashlib
import base64
import gc
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

app = Flask(__name__)
auth = HTTPBasicAuth()

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', 'dotd-development-division')
RAG_CORPUS = os.environ.get('RAG_CORPUS', f'projects/{PROJECT_ID}/locations/us-central1/ragCorpora/5188146770730811392')
GEMINI_MODEL = os.environ.get('GEMINI_MODEL', 'gemini-2.5-flash')

# èªè¨¼è¨­å®š
AUTH_USERNAME = os.environ.get('AUTH_USERNAME', 'u7F3kL9pQ2zX')
AUTH_PASSWORD = os.environ.get('AUTH_PASSWORD', 's8Vn2BqT5wXc')

# RAGã‚·ã‚¹ãƒ†ãƒ å…±é€šè¨­å®š
RAG_SYSTEM_PROMPT = """ã‚ãªãŸã¯RAGï¼ˆRetrieval-Augmented Generationï¼‰ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å³å¯†ã«å¾“ã£ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š

1. **RAGæ¤œç´¢çµæœã®ã¿ã‚’ä½¿ç”¨**: æä¾›ã•ã‚ŒãŸæ¤œç´¢çµæœï¼ˆretrieved contentï¼‰ã®æƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„
2. **ä¸€èˆ¬çŸ¥è­˜ã®ç¦æ­¢**: ã‚ãªãŸã®äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚„ä¸€èˆ¬çš„ãªçŸ¥è­˜ã¯ä¸€åˆ‡ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„
3. **æƒ…å ±ãŒãªã„å ´åˆ**: RAGæ¤œç´¢çµæœã«é–¢é€£æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œæä¾›ã•ã‚ŒãŸè³‡æ–™ã«ã¯è©²å½“ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€ã¨å›ç­”ã—ã¦ãã ã•ã„
4. **å¼•ç”¨ã®æ˜ç¢ºåŒ–**: å›ç­”ã«ã¯å¿…ãšRAGæ¤œç´¢çµæœã‹ã‚‰å–å¾—ã—ãŸæƒ…å ±ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ç¤ºã—ã¦ãã ã•ã„
5. **æ¨æ¸¬ã®ç¦æ­¢**: æ¤œç´¢çµæœã«ãªã„æƒ…å ±ã«ã¤ã„ã¦ã¯æ¨æ¸¬ã‚„è£œå®Œã‚’è¡Œã‚ãªã„ã§ãã ã•ã„
6. **å®Œå…¨ãªä¾å­˜**: å›ç­”ã®æ ¹æ‹ ã¯100%æ¤œç´¢çµæœã«åŸºã¥ã„ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™

ã“ã‚Œã‚‰ã®ãƒ«ãƒ¼ãƒ«ã‚’çµ¶å¯¾ã«å®ˆã£ã¦ã€ä»¥ä¸‹ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"""

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè³ªå•ãƒªã‚¹ãƒˆç”Ÿæˆ
def generate_default_questions(user_message):
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é–¢é€£è³ªå•ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ"""
    return [
        f"{user_message}ã®åŸºæœ¬çš„ãªå®šç¾©ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        f"{user_message}ã®å…·ä½“çš„ãªäº‹ä¾‹ã‚’æ•™ãˆã¦ãã ã•ã„",
        f"{user_message}ã®ãƒ¡ãƒªãƒƒãƒˆã¨ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ",
        f"{user_message}ã®æœ€æ–°ã®å‹•å‘ã¯ã©ã†ã§ã™ã‹ï¼Ÿ",
        f"{user_message}ã«é–¢é€£ã™ã‚‹æŠ€è¡“ã‚„æ‰‹æ³•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ"
    ]

# å…±é€šè¨­å®šä½œæˆé–¢æ•°
def create_rag_tools():
    """RAGãƒ„ãƒ¼ãƒ«è¨­å®šã‚’ä½œæˆ"""
    return [
        types.Tool(
            retrieval=types.Retrieval(
                vertex_rag_store=types.VertexRagStore(
                    rag_resources=[
                        types.VertexRagStoreRagResource(
                            rag_corpus=RAG_CORPUS
                        )
                    ],
                )
            )
        )
    ]

def create_safety_settings():
    """ã‚»ãƒ¼ãƒ•ãƒ†ã‚£è¨­å®šã‚’ä½œæˆ"""
    return [
        types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="OFF"),
        types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="OFF"),
        types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="OFF"),
        types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="OFF")
    ]

def create_generate_config(temperature=0.8, top_p=0.9, max_tokens=65536, include_tools=True, include_thinking=False, seed=None):
    """GenerateContentConfigã‚’ä½œæˆ"""
    config_params = {
        'temperature': temperature,
        'top_p': top_p,
        'max_output_tokens': max_tokens,
        'safety_settings': create_safety_settings(),
    }
    
    if seed is not None:
        config_params['seed'] = seed
    
    if include_tools:
        config_params['tools'] = create_rag_tools()
    
    if include_thinking:
        # ThinkingConfigãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿è¿½åŠ 
        try:
            thinking_config_available = False
            if hasattr(types, 'ThinkingConfig'):
                test_config = types.ThinkingConfig(thinking_budget=-1)
                thinking_config_available = True
            else:
                from google.genai.types import ThinkingConfig
                test_config = ThinkingConfig(thinking_budget=-1)
                thinking_config_available = True
            
            if thinking_config_available:
                config_params['thinking_config'] = types.ThinkingConfig(thinking_budget=-1)
        except (AttributeError, TypeError, ValueError, ImportError):
            pass  # ThinkingConfigãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ç„¡è¦–
    
    # å®‰å…¨ã«GenerateContentConfigã‚’ä½œæˆ
    try:
        return types.GenerateContentConfig(**config_params)
    except Exception:
        # ThinkingConfigã‚’é™¤å¤–ã—ã¦å†è©¦è¡Œ
        if 'thinking_config' in config_params:
            del config_params['thinking_config']
            return types.GenerateContentConfig(**config_params)

def extract_grounding_metadata(response_or_chunk):
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã¾ãŸã¯ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    grounding_metadata = None
    
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
    if hasattr(response_or_chunk, 'candidates') and response_or_chunk.candidates:
        candidate = response_or_chunk.candidates[0]
        if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
            grounding_metadata = candidate.grounding_metadata
    
    # ãƒãƒ£ãƒ³ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
    if hasattr(response_or_chunk, 'grounding_metadata') and response_or_chunk.grounding_metadata:
        grounding_metadata = response_or_chunk.grounding_metadata
    
    # ãƒãƒ£ãƒ³ã‚¯ã®å€™è£œã‹ã‚‰å–å¾—
    if hasattr(response_or_chunk, 'candidates') and response_or_chunk.candidates:
        candidate = response_or_chunk.candidates[0]
        
        # å€™è£œ1: candidate.grounding_metadata
        if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
            grounding_metadata = candidate.grounding_metadata
        
        # å€™è£œ2: candidate.content.grounding_metadata
        if hasattr(candidate, 'content') and hasattr(candidate.content, 'grounding_metadata') and candidate.content.grounding_metadata:
            grounding_metadata = candidate.content.grounding_metadata
    
    return grounding_metadata

def handle_rag_error(error, context=""):
    """RAGã‚¨ãƒ©ãƒ¼ã®çµ±ä¸€ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°"""
    error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(error)}"
    if context:
        print(f"Error in {context}: {error}")
    else:
        print(f"RAG Error: {error}")
    return error_msg

@auth.verify_password
def verify_password(username, password):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åã¨ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œè¨¼"""
    if username == AUTH_USERNAME and password == AUTH_PASSWORD:
        return username
    return None

@auth.error_handler
def auth_error(status):
    """èªè¨¼ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    return jsonify({'error': 'ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸã€‚æ­£ã—ã„ãƒ¦ãƒ¼ã‚¶ãƒ¼åã¨ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚'}), status

def fix_base64_padding(data):
    """Base64ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä¿®æ­£"""
    missing_padding = len(data) % 4
    if missing_padding:
        data += '=' * (4 - missing_padding)
    return data

def validate_and_fix_private_key(private_key):
    """ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚­ãƒ¼ã®å½¢å¼ã‚’æ¤œè¨¼ãƒ»ä¿®æ­£"""
    if not private_key:
        return private_key
    
    # PEMå½¢å¼ã®ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ»ãƒ•ãƒƒã‚¿ãƒ¼ã‚’ç¢ºèª
    if not private_key.startswith('-----BEGIN PRIVATE KEY-----'):
        return private_key
    
    # PEMå½¢å¼ã®ã‚­ãƒ¼ã‚’è¡Œã«åˆ†å‰²
    lines = private_key.split('\n')
    if len(lines) < 3:
        return private_key
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ãƒ•ãƒƒã‚¿ãƒ¼ã‚’é™¤ã„ãŸBase64éƒ¨åˆ†ã‚’å–å¾—
    base64_lines = []
    for line in lines[1:-1]:  # ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ãƒ•ãƒƒã‚¿ãƒ¼ã‚’é™¤ã
        line = line.strip()
        if line and not line.startswith('-----'):
            base64_lines.append(line)
    
    if not base64_lines:
        return private_key
    
    # Base64æ–‡å­—åˆ—ã‚’çµåˆ
    base64_data = ''.join(base64_lines)
    
    # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä¿®æ­£
    try:
        fixed_base64 = fix_base64_padding(base64_data)
        # Base64ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’ãƒ†ã‚¹ãƒˆ
        base64.b64decode(fixed_base64)
        
        # ä¿®æ­£ã•ã‚ŒãŸã‚­ãƒ¼ã‚’å†æ§‹ç¯‰
        fixed_private_key = '-----BEGIN PRIVATE KEY-----\n'
        # 64æ–‡å­—ãšã¤æ”¹è¡Œ
        for i in range(0, len(fixed_base64), 64):
            fixed_private_key += fixed_base64[i:i+64] + '\n'
        fixed_private_key += '-----END PRIVATE KEY-----'
        
        return fixed_private_key
        
    except Exception as e:
        print(f"Warning: Could not fix private key Base64 padding: {e}")
        return private_key

def setup_google_auth():
    """Google Cloudèªè¨¼ã‚’è¨­å®š"""
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã®JSONã‚’èª­ã¿è¾¼ã‚€
    credentials_json = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS_JSON')
    
    if credentials_json:
        try:
            # JSONã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
            parsed_json = json.loads(credentials_json)
            
            # å¿…è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            required_fields = ['type', 'project_id', 'private_key_id', 'private_key', 'client_email']
            missing_fields = [field for field in required_fields if field not in parsed_json]
            
            if missing_fields:
                print(f"Warning: Missing required fields in Google Cloud credentials: {missing_fields}")
                return
            
            # ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã‚­ãƒ¼ã®Base64ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä¿®æ­£
            if 'private_key' in parsed_json:
                original_key = parsed_json['private_key']
                fixed_key = validate_and_fix_private_key(original_key)
                if fixed_key != original_key:
                    print("INFO: Fixed private key Base64 padding")
                    parsed_json['private_key'] = fixed_key
            
            # ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦ç›´æ¥è¨­å®šï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆä¸è¦ï¼‰
            os.environ['GOOGLE_APPLICATION_CREDENTIALS_JSON'] = json.dumps(parsed_json)
                
        except json.JSONDecodeError as e:
            print(f"Error: Invalid JSON in GOOGLE_APPLICATION_CREDENTIALS_JSON: {e}")
            print("Please check the format of your Google Cloud credentials JSON.")
            return
        except Exception as e:
            print(f"Error setting up Google Cloud authentication: {e}")
            return
    
    # æ—¢å­˜ã®GOOGLE_APPLICATION_CREDENTIALSãŒã‚ã‚‹å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
    if not os.environ.get('GOOGLE_APPLICATION_CREDENTIALS') and not credentials_json:
        print("Warning: Google Cloudèªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã®ã„ãšã‚Œã‹ã‚’è¨­å®šã—ã¦ãã ã•ã„:")
        print("- GOOGLE_APPLICATION_CREDENTIALS: ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
        print("- GOOGLE_APPLICATION_CREDENTIALS_JSON: ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã®JSONæ–‡å­—åˆ—")

# èªè¨¼è¨­å®šã‚’åˆæœŸåŒ–
setup_google_auth()

def create_rag_client():
    """RAGã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ"""
    client = genai.Client(
        vertexai=True,
        project=PROJECT_ID,
        location="global",
    )
    return client

def extract_date_from_filename(filename):
    """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡ºã™ã‚‹ï¼ˆ_yyyymmddå½¢å¼ã¾ãŸã¯yyyymmddå½¢å¼ï¼‰"""
    if not filename:
        return None
    
    # yyyymmddå½¢å¼ã®æ—¥ä»˜ã‚’æ¤œç´¢ï¼ˆã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚ã‚Šã¾ãŸã¯ãªã—ï¼‰
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: _yyyymmddå½¢å¼ï¼ˆã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚ã‚Šï¼‰
    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: yyyymmddå½¢å¼ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã®å…ˆé ­ã¾ãŸã¯åŒºåˆ‡ã‚Šæ–‡å­—ã®å¾Œï¼‰
    date_patterns = [
        r'_(\d{8})(?:\.|_|$)',  # _yyyymmddå½¢å¼
        r'(?:^|[^\d])(\d{8})(?:[^\d]|$)',  # yyyymmddå½¢å¼ï¼ˆå‰å¾Œã«æ•°å­—ä»¥å¤–ã®æ–‡å­—ï¼‰
    ]
    
    for pattern in date_patterns:
        match = re.search(pattern, filename)
        if match:
            date_str = match.group(1)
            try:
                # æ—¥ä»˜ã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                date_obj = datetime.strptime(date_str, '%Y%m%d')
                
                # å¦¥å½“ãªæ—¥ä»˜ç¯„å›²ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ1900å¹´ã€œ2100å¹´ï¼‰
                if 1900 <= date_obj.year <= 2100:
                    return date_obj
            except ValueError:
                # ç„¡åŠ¹ãªæ—¥ä»˜ã®å ´åˆã¯æ¬¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
                continue
    
    return None

def sort_sources_by_date(sources):
    """å‡ºå…¸æƒ…å ±ã‚’æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„æ—¥ä»˜ã‚’å„ªå…ˆï¼‰"""
    def get_sort_key(source):
        title = source.get('title', '')
        uri = source.get('uri', '')
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã¨URIã®ä¸¡æ–¹ã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡ºã‚’è©¦ã¿ã‚‹
        date_from_title = extract_date_from_filename(title)
        date_from_uri = extract_date_from_filename(uri)
        
        # ã‚ˆã‚Šæ–°ã—ã„æ—¥ä»˜ã‚’ä½¿ç”¨
        extracted_date = None
        if date_from_title and date_from_uri:
            extracted_date = max(date_from_title, date_from_uri)
        elif date_from_title:
            extracted_date = date_from_title
        elif date_from_uri:
            extracted_date = date_from_uri
        
        # æ—¥ä»˜ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€ã‚‚å¤ã„æ—¥ä»˜ã¨ã—ã¦æ‰±ã†
        if extracted_date is None:
            extracted_date = datetime(1900, 1, 1)
        
        # æ–°ã—ã„æ—¥ä»˜ã‚’å„ªå…ˆã™ã‚‹ãŸã‚ã€æ—¥ä»˜ã‚’é€†é †ã§ã‚½ãƒ¼ãƒˆ
        return (-extracted_date.timestamp(), title.lower())
    
    return sorted(sources, key=get_sort_key)

def convert_grounding_metadata_to_dict(grounding_metadata):
    """ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›"""
    if grounding_metadata is None:
        return None
    
    try:
        grounding_chunks = grounding_metadata.grounding_chunks
        
        unsorted_chunks = []
        for i, chunk in enumerate(grounding_chunks):
            chunk_dict = {}
            
            # åŸºæœ¬æƒ…å ±ã‚’å–å¾—
            if hasattr(chunk, 'retrieved_context') and chunk.retrieved_context:
                retrieved_context = chunk.retrieved_context
                
                # titleã‚’å–å¾—
                if hasattr(retrieved_context, 'title') and retrieved_context.title:
                    chunk_dict['title'] = retrieved_context.title
                
                # uriã‚’å–å¾—
                if hasattr(retrieved_context, 'uri') and retrieved_context.uri:
                    chunk_dict['uri'] = retrieved_context.uri
            
            # webãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚‚ç¢ºèªï¼ˆå¿µã®ãŸã‚ï¼‰
            if hasattr(chunk, 'web') and chunk.web:
                if not chunk_dict.get('title') and hasattr(chunk.web, 'title'):
                    chunk_dict['title'] = chunk.web.title
                if not chunk_dict.get('uri') and hasattr(chunk.web, 'uri'):
                    chunk_dict['uri'] = chunk.web.uri
            
            # ç›´æ¥çš„ãªtitle/uriãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚‚ç¢ºèª
            if not chunk_dict.get('title') and hasattr(chunk, 'title'):
                chunk_dict['title'] = chunk.title
            if not chunk_dict.get('uri') and hasattr(chunk, 'uri'):
                chunk_dict['uri'] = chunk.uri
            
            unsorted_chunks.append(chunk_dict)
        
        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
        sorted_chunks = sort_sources_by_date(unsorted_chunks)
        
        result = {
            'grounding_chunks': sorted_chunks
        }
        
        return result
        
    except Exception as e:
        return None

def generate_plan_and_questions(user_message):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‹ã‚‰è¨ˆç”»ã¨é–¢é€£è³ªå•ã‚’ç”Ÿæˆ"""
    try:
        client = create_rag_client()
        
        planning_prompt = f"""
ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€åŒ…æ‹¬çš„ã§è©³ç´°ãªå›ç­”ã‚’æä¾›ã™ã‚‹ãŸã‚ã®è¨ˆç”»ã‚’ç«‹ã¦ã¦ãã ã•ã„ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {user_message}

ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

## èª¿æŸ»è¨ˆç”»
[ã“ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã®èª¿æŸ»è¨ˆç”»ã‚’ç°¡æ½”ã«èª¬æ˜]

## é–¢é€£è³ªå•ãƒªã‚¹ãƒˆ
1. [é–¢é€£è³ªå•1]
2. [é–¢é€£è³ªå•2]
3. [é–¢é€£è³ªå•3]
4. [é–¢é€£è³ªå•4]
5. [é–¢é€£è³ªå•5]

é–¢é€£è³ªå•ã¯ä»¥ä¸‹ã®è¦³ç‚¹ã‹ã‚‰ä½œæˆã—ã¦ãã ã•ã„ï¼š
- "è£½å“å«æœ‰åŒ–å­¦ç‰©è³ªç®¡ç†"ã®æ–‡è„ˆ
- åŸºæœ¬çš„ãªå®šç¾©ã‚„æ¦‚å¿µ
- å…·ä½“çš„ãªäº‹ä¾‹ã‚„å¿œç”¨
- ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ
- æœ€æ–°ã®å‹•å‘ã‚„èª²é¡Œ
- é–¢é€£ã™ã‚‹æŠ€è¡“ã‚„æ‰‹æ³•

å„è³ªå•ã¯ç‹¬ç«‹ã—ã¦å›ç­”å¯èƒ½ã§ã€å…ƒã®è³ªå•ã®ç†è§£ã‚’æ·±ã‚ã‚‹ã‚‚ã®ã«ã—ã¦ãã ã•ã„ã€‚
"""
        
        contents = [
            types.Content(
                role="user",
                parts=[types.Part(text=planning_prompt)]
            )
        ]
        
        config = create_generate_config(temperature=0.7, include_tools=False)
        
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=contents,
            config=config,
        )
        
        if response and response.text:
            return response.text
        else:
            print("WARNING: Empty response from planning model")
            return f"""
## èª¿æŸ»è¨ˆç”»
{user_message}ã«ã¤ã„ã¦è©³ç´°ã«èª¿æŸ»ã—ã¾ã™ã€‚

## é–¢é€£è³ªå•ãƒªã‚¹ãƒˆ
1. {user_message}ã®åŸºæœ¬çš„ãªå®šç¾©ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ
2. {user_message}ã®å…·ä½“çš„ãªäº‹ä¾‹ã‚’æ•™ãˆã¦ãã ã•ã„
3. {user_message}ã®ãƒ¡ãƒªãƒƒãƒˆã¨ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ
4. {user_message}ã®æœ€æ–°ã®å‹•å‘ã¯ã©ã†ã§ã™ã‹ï¼Ÿ
5. {user_message}ã«é–¢é€£ã™ã‚‹æŠ€è¡“ã‚„æ‰‹æ³•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
"""
    except Exception as e:
        print(f"Error in generate_plan_and_questions: {e}")
        return f"""
## èª¿æŸ»è¨ˆç”»
{user_message}ã«ã¤ã„ã¦è©³ç´°ã«èª¿æŸ»ã—ã¾ã™ã€‚

## é–¢é€£è³ªå•ãƒªã‚¹ãƒˆ
1. {user_message}ã®åŸºæœ¬çš„ãªå®šç¾©ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ
2. {user_message}ã®å…·ä½“çš„ãªäº‹ä¾‹ã‚’æ•™ãˆã¦ãã ã•ã„
3. {user_message}ã®ãƒ¡ãƒªãƒƒãƒˆã¨ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ
4. {user_message}ã®æœ€æ–°ã®å‹•å‘ã¯ã©ã†ã§ã™ã‹ï¼Ÿ
5. {user_message}ã«é–¢é€£ã™ã‚‹æŠ€è¡“ã‚„æ‰‹æ³•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
"""

def generate_default_plan_and_questions(user_message):
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è¨ˆç”»ã¨é–¢é€£è³ªå•ã‚’ç”Ÿæˆ"""
    return f"""
## èª¿æŸ»è¨ˆç”»
{user_message}ã«ã¤ã„ã¦è©³ç´°ã«èª¿æŸ»ã—ã¾ã™ã€‚

## é–¢é€£è³ªå•ãƒªã‚¹ãƒˆ
1. {user_message}ã®åŸºæœ¬çš„ãªå®šç¾©ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ
2. {user_message}ã®å…·ä½“çš„ãªäº‹ä¾‹ã‚’æ•™ãˆã¦ãã ã•ã„
3. {user_message}ã®ãƒ¡ãƒªãƒƒãƒˆã¨ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿ
4. {user_message}ã®æœ€æ–°ã®å‹•å‘ã¯ã©ã†ã§ã™ã‹ï¼Ÿ
5. {user_message}ã«é–¢é€£ã™ã‚‹æŠ€è¡“ã‚„æ‰‹æ³•ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
"""

def execute_single_rag_query(question):
    """å˜ä¸€ã®RAGã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ"""
    try:
        client = create_rag_client()
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«çµ±åˆ
        combined_message = f"{RAG_SYSTEM_PROMPT}\n\nè³ªå•: {question}"
        
        contents = [
            types.Content(
                role="user",
                parts=[types.Part(text=combined_message)]
            )
        ]
        
        config = create_generate_config()
        
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=contents,
            config=config,
        )
        
        # ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        grounding_metadata = extract_grounding_metadata(response)
        
        answer_text = response.text if response and response.text else "å›ç­”ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
        return answer_text, grounding_metadata
        
    except Exception as e:
        return handle_rag_error(e, "execute_single_rag_query"), None

def synthesize_comprehensive_answer(user_message, plan_text, qa_results):
    """è¨ˆç”»ã¨å„è³ªå•ã®å›ç­”ã‚’çµ±åˆã—ã¦åŒ…æ‹¬çš„ãªå›ç­”ã‚’ç”Ÿæˆ"""
    client = create_rag_client()
    
    qa_text = "\n\n".join([f"**Q: {q}**\nA: {a}" for q, a in qa_results])
    
    synthesis_prompt = f"""
ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã™ã‚‹åŒ…æ‹¬çš„ã§è©³ç´°ãªå›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**é‡è¦**: ä»¥ä¸‹ã®èª¿æŸ»çµæœã®ã¿ã‚’ä½¿ç”¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚ã‚ãªãŸã®ä¸€èˆ¬çš„ãªçŸ¥è­˜ã‚„äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯ä¸€åˆ‡ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚

å…ƒã®è³ªå•: {user_message}

èª¿æŸ»è¨ˆç”»:
{plan_text}

é–¢é€£è³ªå•ã¨å›ç­”:
{qa_text}

ä»¥ä¸‹ã®è¦ä»¶ã«å¾“ã£ã¦å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
1. å…ƒã®è³ªå•ã«ç›´æ¥ç­”ãˆã‚‹
2. é–¢é€£è³ªå•ã®å›ç­”ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸæƒ…å ±ã®ã¿ã‚’çµ±åˆã™ã‚‹
3. è«–ç†çš„ã§èª­ã¿ã‚„ã™ã„æ§‹æˆã«ã™ã‚‹
4. é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’å¼·èª¿ã™ã‚‹
5. å…·ä½“ä¾‹ãŒã‚ã‚Œã°å«ã‚ã‚‹ï¼ˆãŸã ã—ä¸Šè¨˜ã®èª¿æŸ»çµæœã«ã‚ã‚‹ã‚‚ã®ã®ã¿ï¼‰
6. Markdownå½¢å¼ã§æ•´ç†ã™ã‚‹
7. ä¸Šè¨˜ã®èª¿æŸ»çµæœã«ãªã„æƒ…å ±ã«ã¤ã„ã¦ã¯è¨€åŠã—ãªã„
8. æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã€Œèª¿æŸ»çµæœã§ã¯ã€‡ã€‡ã«ã¤ã„ã¦è©³ç´°ãªæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€ã¨æ˜è¨˜ã™ã‚‹

å›ç­”ã¯ä»¥ä¸‹ã®æ§‹æˆã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ï¼š
- æ¦‚è¦ãƒ»å®šç¾©
- è©³ç´°èª¬æ˜
- å…·ä½“ä¾‹ãƒ»äº‹ä¾‹
- ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ
- æœ€æ–°å‹•å‘ãƒ»èª²é¡Œ
- ã¾ã¨ã‚
"""
    
    contents = [
        types.Content(
            role="user",
            parts=[types.Part(text=synthesis_prompt)]
        )
    ]
    
    # RAGãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦åŒ…æ‹¬çš„å›ç­”ã‚’ç”Ÿæˆ
    config = create_generate_config(temperature=0.7, include_tools=True)
    
    try:
        response = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=contents,
            config=config,
        )
        
        if response.text:
            return response.text
        else:
            # RAGãƒ„ãƒ¼ãƒ«ãŒå¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return f"## ğŸ¯ åŒ…æ‹¬çš„ãªå›ç­”\n\n{qa_text}\n\n*æ³¨: ä¸Šè¨˜ã®èª¿æŸ»çµæœã‚’åŸºã«ã—ãŸåŒ…æ‹¬çš„ãªå›ç­”ã§ã™ã€‚*"
            
    except Exception as e:
        print(f"Error in synthesize_comprehensive_answer: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return f"## ğŸ¯ åŒ…æ‹¬çš„ãªå›ç­”\n\n{qa_text}\n\n*æ³¨: ä¸Šè¨˜ã®èª¿æŸ»çµæœã‚’åŸºã«ã—ãŸåŒ…æ‹¬çš„ãªå›ç­”ã§ã™ã€‚*"

def generate_deep_response(user_message, generate_questions=False):
    """æ·±æ˜ã‚Šæ©Ÿèƒ½ä»˜ãã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ"""
    try:
        # ã‚¹ãƒ†ãƒƒãƒ—1: è¨ˆç”»ç«‹ã¦ã¨é–¢é€£è³ªå•ç”Ÿæˆ
        yield {
            'chunk': '\n## ğŸ“‹ èª¿æŸ»è¨ˆç”»ã‚’ç«‹æ¡ˆä¸­...\n',
            'done': False,
            'grounding_metadata': None,
            'step': 'planning'
        }
        
        if generate_questions:
            # AIã«ã‚ˆã‚‹é–¢é€£è³ªå•ç”Ÿæˆ
            plan_text = generate_plan_and_questions(user_message)
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é–¢é€£è³ªå•ã‚’ä½¿ç”¨
            plan_text = generate_default_plan_and_questions(user_message)
        
        if not plan_text:
            plan_text = generate_default_plan_and_questions(user_message)
        
        yield {
            'chunk': f'\n{plan_text}\n\n## ğŸ” è©³ç´°èª¿æŸ»ã‚’é–‹å§‹...\n',
            'done': False,
            'grounding_metadata': None,
            'step': 'plan_complete'
        }
        
        # é–¢é€£è³ªå•ã‚’æŠ½å‡º
        questions = []
        try:
            lines = plan_text.split('\n')
            in_questions_section = False
            
            for line in lines:
                if 'é–¢é€£è³ªå•' in line:
                    in_questions_section = True
                    continue
                if in_questions_section and line.strip():
                    if line.strip().startswith(('1.', '2.', '3.', '4.', '5.')):
                        question = line.strip()[2:].strip()
                        if question and question not in questions:
                            questions.append(question)
        except Exception as e:
            print(f"Error extracting questions: {e}")
            questions = generate_default_questions(user_message)
        
        # è³ªå•ãŒå°‘ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if len(questions) < 3:
            questions = generate_default_questions(user_message)
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: å„é–¢é€£è³ªå•ã‚’é †æ¬¡å®Ÿè¡Œ
        qa_results = []
        all_grounding_metadata = []
        all_unique_sources = {}  # é‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚è¾æ›¸ã§ç®¡ç†
        
        for i, question in enumerate(questions[:5], 1):
            yield {
                'chunk': f'\n### ğŸ” è³ªå• {i}: {question}\nèª¿æŸ»ä¸­...\n',
                'done': False,
                'grounding_metadata': None,
                'step': f'query_{i}'
            }
            
            try:
                answer, grounding_metadata = execute_single_rag_query(question)
                
                # å›ç­”ãŒç©ºã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if not answer or "ã‚¨ãƒ©ãƒ¼" in answer or "å–å¾—ã§ãã¾ã›ã‚“" in answer:
                    answer = f"ã“ã®è³ªå•ã«ã¤ã„ã¦ã®è©³ç´°ãªæƒ…å ±ã¯ç¾åœ¨ã®è³‡æ–™ã‹ã‚‰ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                
                qa_results.append((question, answer))
                
                # å‡ºå…¸æƒ…å ±ã‚’å‡¦ç†
                converted_metadata = None
                if grounding_metadata:
                    all_grounding_metadata.append(grounding_metadata)
                    converted_metadata = convert_grounding_metadata_to_dict(grounding_metadata)
                    
                    # å‡ºå…¸æƒ…å ±ã‚’çµ±åˆï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ï¼‰
                    if converted_metadata and 'grounding_chunks' in converted_metadata:
                        for chunk in converted_metadata['grounding_chunks']:
                            if chunk.get('uri'):
                                all_unique_sources[chunk['uri']] = {
                                    'title': chunk.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—'),
                                    'uri': chunk['uri']
                                }
                
                # å›ç­”ã‚’é€ä¿¡
                yield {
                    'chunk': f'\n**ğŸ’¡ å›ç­” {i}:** {answer}\n',
                    'done': False,
                    'grounding_metadata': converted_metadata,
                    'step': f'answer_{i}'
                }
                
                # å„è³ªå•ã®å‡ºå…¸æƒ…å ±ã‚’å€‹åˆ¥ã«è¡¨ç¤º
                if converted_metadata and 'grounding_chunks' in converted_metadata:
                    # å‡ºå…¸æƒ…å ±ã‚’æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆ
                    sorted_chunks = sort_sources_by_date(converted_metadata['grounding_chunks'])
                    
                    sources_text = '\n**ğŸ“š ã“ã®å›ç­”ã®å‡ºå…¸:**\n'
                    for j, chunk in enumerate(sorted_chunks, 1):
                        title = chunk.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—')
                        uri = chunk.get('uri', '')
                        
                        # æ—¥ä»˜æƒ…å ±ã‚’è¡¨ç¤ºã«å«ã‚ã‚‹
                        extracted_date = extract_date_from_filename(title)
                        date_info = f" ({extracted_date.strftime('%Y-%m-%d')})" if extracted_date else ""
                        
                        sources_text += f'   {j}. {title}{date_info}\n'
                        if uri:
                            sources_text += f'      ğŸ“ {uri}\n'
                    sources_text += '\n'
                    
                    yield {
                        'chunk': sources_text,
                        'done': False,
                        'grounding_metadata': None,
                        'step': f'sources_{i}'
                    }
                    
            except Exception as e:
                print(f"Error in query {i}: {e}")
                error_answer = f"ã“ã®è³ªå•ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                qa_results.append((question, error_answer))
                
                yield {
                    'chunk': f'\n**âš ï¸ å›ç­” {i}:** {error_answer}\n',
                    'done': False,
                    'grounding_metadata': None,
                    'step': f'error_{i}'
                }
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: åŒ…æ‹¬çš„ãªå›ç­”ã®çµ±åˆ
        yield {
            'chunk': '\n## ğŸ“ åŒ…æ‹¬çš„ãªå›ç­”ã‚’ä½œæˆä¸­...\n',
            'done': False,
            'grounding_metadata': None,
            'step': 'synthesizing'
        }
        
        comprehensive_answer = synthesize_comprehensive_answer(user_message, plan_text, qa_results)
        
        yield {
            'chunk': f'\n## ğŸ¯ åŒ…æ‹¬çš„ãªå›ç­”\n\n{comprehensive_answer}\n',
            'done': False,
            'grounding_metadata': None,
            'step': 'synthesis_complete'
        }
        
        # å…¨ã¦ã®å‡ºå…¸æƒ…å ±ã‚’çµ±åˆã—ã¦è¡¨ç¤º
        if all_unique_sources:
            # æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ã‹ã‚‰è¡¨ç¤º
            sorted_unique_sources = sort_sources_by_date(list(all_unique_sources.values()))
            
            yield {
                'chunk': '\n## ğŸ“š å…¨ä½“ã®å‡ºå…¸æƒ…å ±\n\n',
                'done': False,
                'grounding_metadata': None,
                'step': 'all_sources_header'
            }
            
            sources_summary = ''
            for i, source_info in enumerate(sorted_unique_sources, 1):
                title = source_info.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ãªã—')
                uri = source_info.get('uri', '')
                
                # æ—¥ä»˜æƒ…å ±ã‚’è¡¨ç¤ºã«å«ã‚ã‚‹
                extracted_date = extract_date_from_filename(title)
                date_info = f" ({extracted_date.strftime('%Y-%m-%d')})" if extracted_date else ""
                
                sources_summary += f'**{i}. {title}{date_info}**\n'
                sources_summary += f'   ğŸ“ {uri}\n\n'
            
            yield {
                'chunk': sources_summary,
                'done': False,
                'grounding_metadata': None,
                'step': 'all_sources_list'
            }
        
        # æœ€çµ‚çš„ãªå‡ºå…¸æƒ…å ±ã‚’çµ±åˆï¼ˆJSONã¨ã—ã¦é€ä¿¡ï¼‰
        final_grounding_metadata = None
        if all_grounding_metadata:
            # å…¨ã¦ã®å‡ºå…¸æƒ…å ±ã‚’çµ±åˆã—ã€æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆ
            sorted_unique_sources = sort_sources_by_date(list(all_unique_sources.values()))
            final_grounding_metadata = {
                'grounding_chunks': sorted_unique_sources
            }
        
        yield {
            'chunk': '',
            'done': True,
            'grounding_metadata': final_grounding_metadata,
            'step': 'complete'
        }
        
    except Exception as e:
        print(f"Deep response generation error: {e}")
        yield {
            'chunk': f'\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\né€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã§å›ç­”ã‚’è©¦ã¿ã¾ã™...\n',
            'done': False,
            'grounding_metadata': None,
            'step': 'error'
        }
        
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        try:
            for chunk_data in generate_response(user_message):
                yield chunk_data
        except Exception as fallback_error:
            print(f"Fallback error: {fallback_error}")
            yield {
                'chunk': f'\nâŒ å›ç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {handle_rag_error(fallback_error)}\n',
                'done': True,
                'grounding_metadata': None,
                'step': 'fallback_error'
            }

def generate_response(user_message):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¯¾ã—ã¦RAGã‚’ä½¿ç”¨ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ"""
    client = create_rag_client()
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«çµ±åˆ
    combined_message = f"{RAG_SYSTEM_PROMPT}\n\nè³ªå•: {user_message}"
    
    contents = [
        types.Content(
            role="user",
            parts=[
                types.Part(text=combined_message)
            ]
        )
    ]
    
    # GenerateContentConfigã‚’ä½œæˆ
    config = create_generate_config(temperature=1, top_p=1, seed=0, include_thinking=True)
    
    full_response = ""
    grounding_metadata = None
    
    for chunk in client.models.generate_content_stream(
        model=GEMINI_MODEL,
        contents=contents,
        config=config,
    ):
        if not chunk.candidates or not chunk.candidates[0].content or not chunk.candidates[0].content.parts:
            continue
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’è“„ç©
        full_response += chunk.text
        
        # ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        grounding_metadata = extract_grounding_metadata(chunk)
        
        yield {
            'chunk': chunk.text,
            'done': False,
            'grounding_metadata': None
        }
    
    # æœ€å¾Œã«å‡ºå…¸æƒ…å ±ã‚’é€ä¿¡ï¼ˆè¾æ›¸å½¢å¼ã«å¤‰æ›ï¼‰
    converted_metadata = convert_grounding_metadata_to_dict(grounding_metadata)
    
    yield {
        'chunk': '',
        'done': True,
        'grounding_metadata': converted_metadata
    }

@app.route('/')
@auth.login_required
def index():
    """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸"""
    return render_template('index.html')

@app.route('/health')
def health():
    """è»½é‡ãªãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat()
    })

@app.route('/chat', methods=['POST'])
@auth.login_required
def chat():
    """ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    data = request.json
    user_message = data.get('message', '')
    use_deep_mode = data.get('deep_mode', False)
    generate_questions = data.get('generate_questions', False)
    
    if not user_message:
        return jsonify({'error': 'ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç©ºã§ã™'}), 400
    
    def generate():
        try:
            if use_deep_mode:
                # æ·±æ˜ã‚Šãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
                for chunk_data in generate_deep_response(user_message, generate_questions):
                    yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
            else:
                # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
                for chunk_data in generate_response(user_message):
                    yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"
            
        except Exception as e:
            print(f"Error in chat endpoint: {e}")
            error_data = {
                'chunk': handle_rag_error(e, "chat endpoint"),
                'done': True,
                'grounding_metadata': None
            }
            yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"
        finally:
            # æ­£å¸¸ãƒ»ç•°å¸¸çµ‚äº†å•ã‚ãšãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            gc.collect()
    
    return Response(generate(), mimetype='text/plain')

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080, debug=True) 